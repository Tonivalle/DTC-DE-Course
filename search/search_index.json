{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This repository is for the 2023 DTC Data Engineering course.</p> <p>https://github.com/DataTalksClub/data-engineering-zoomcamp</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Week 1: Introduction &amp; Prerequisites</li> <li> Week 2: Workflow Orchestration</li> <li> Week 3: Data Warehouse</li> <li> Week 4: Analytics engineering</li> <li> Week 5: Batch processing</li> <li> Week 6: Streaming</li> <li> Week 7, 8 &amp; 9: Project</li> </ul>"},{"location":"week_1/1-google-cloud/","title":"Google Cloud","text":""},{"location":"week_1/1-google-cloud/#1-sing-up-to-google-cloud","title":"1. Sing up to Google Cloud","text":"<p>The Cloud provider used for this project will be Google Cloud.</p> <p>https://console.cloud.google.com/getting-started</p>"},{"location":"week_1/1-google-cloud/#2-install-google-cloud-cli","title":"2. Install Google Cloud CLI","text":"<p>Follow the installation guide to install the CLI https://cloud.google.com/sdk/docs/install</p> <p>The CLI commands are here.</p>"},{"location":"week_1/1-google-cloud/#3-setup-project","title":"3. Setup project","text":""},{"location":"week_1/1-google-cloud/#set-up-adc-application-default-credentials","title":"Set up ADC (Application Default Credentials)","text":"<p>You need to set-up ADC for the environment you are going to use. To use your own account: <pre><code>gcloud auth application-default login\n</code></pre> Export the variable to the environment once you have logged in (The following path is the default for UNIX): <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.config/gcloud/application_default_credentials.json\"\n</code></pre> On this tutorial we will use a service account.</p> <p>To create a service account, you have to got to:</p> <p></p> <p>In there, we create a new service account with Storage Admin + Storage Object Admin + BigQuery Admin permissions.</p> <p>We generate a key for that account by clicking the 3 dots and adding a key.</p> <p></p> <p>Once it downloads, you can export it the same as your personal account and log in using that account. <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"&lt;path/to/your/service-account-authkeys&gt;.json\"\n\ngcloud auth activate-service-account --key-file \"&lt;path/to/your/service-account-authkeys&gt;.json\"\n</code></pre> You should now see the account if you do: <pre><code>gcloud auth list\n</code></pre> and can now activate that account with: <pre><code>gcloud config set account &lt;accountemailaddress&gt;\n</code></pre></p> <p>For more info you can check the Tutorial.</p> <p>Enable these APIs for your project (by clicking on the links):</p> <p>https://console.cloud.google.com/apis/library/iam.googleapis.com https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com https://console.cloud.google.com/apis/library/bigquery.googleapis.com</p> <p>Once we do that we can install python client libraries. e.g.: <pre><code>poetry add google-cloud-storage\n</code></pre> or  <pre><code>pip install --upgrade google-cloud-storage\n</code></pre></p>"},{"location":"week_1/2-terraform/","title":"Terraform","text":""},{"location":"week_1/2-terraform/#1-install-terraform","title":"1. Install Terraform","text":"<p>Terraform is an open-source infrastructure as code software tool. It will be used to generate the GCP infrastructure. <pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre></p>"},{"location":"week_1/2-terraform/#other-options","title":"Other options","text":"<p>https://developer.hashicorp.com/terraform/downloads</p>"},{"location":"week_1/2-terraform/#terraform-gcp-get-started-guide","title":"Terraform GCP get started guide","text":"<p>https://developer.hashicorp.com/terraform/tutorials/gcp-get-started</p>"},{"location":"week_1/2-terraform/#2-code-your-infrastructure-with-terraform","title":"2. Code your infrastructure with Terraform","text":""},{"location":"week_1/2-terraform/#maintf","title":"main.tf","text":"<p>This file contains these 4 basic declarations: * <code>terraform</code>: configure basic Terraform settings to provision your infrastructure    * <code>required_version</code>: minimum Terraform version to apply to your configuration    * <code>backend</code>: stores Terraform's \"state\" snapshots, to map real-world resources to your configuration.       * <code>local</code>: stores state file locally as <code>terraform.tfstate</code>    * <code>required_providers</code>: specifies the providers required by the current module * <code>provider</code>:    * adds a set of resource types and/or data sources that Terraform can manage    * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms. * <code>resource</code>   * Physical component and its attributes.   * blocks to define components of your infrastructure   * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table</p> <p>Note</p> <p>The resources change name with the different providers; in this case the required_providers source is <code>hashicorp/google</code> so we get <code>google_storage_bucket</code>for example. If it was <code>aws</code> we would have to use the <code>s3</code> version of the package.</p>"},{"location":"week_1/2-terraform/#variablestf","title":"variables.tf","text":"<ul> <li><code>variable</code> &amp; <code>locals</code> are runtime arguments and constants</li> </ul> <p>They are used in the <code>main.tf</code> like so:</p> <p>variables.tf <pre><code>variable \"project\" {\ndescription = \"Your GCP Project ID\"\n}\n\nvariable \"region\" {\ndescription = \"Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations\"\ndefault = \"europe-west6\"\ntype = string\n}\n</code></pre> main.tf: <pre><code>provider \"google\" {\nproject = var.project\nregion = var.region\n}\n</code></pre></p>"},{"location":"week_1/2-terraform/#3-generate-your-infrastructure-via-terraform","title":"3. Generate your infrastructure via Terraform","text":"<p>Terraform has few associated commands  that we need to use:</p>"},{"location":"week_1/2-terraform/#initialize-state-file-tfstate","title":"Initialize state file (.tfstate)","text":"<p><pre><code>terraform init\n</code></pre> </p>"},{"location":"week_1/2-terraform/#check-changes-to-new-infra-plan","title":"Check changes to new infra plan","text":"<p>This command matches changes against the previous state. <pre><code>terraform plan -var=\"project=&lt;your-gcp-project-id&gt;\"\n</code></pre> If you dont add the <code>-var</code> argument, You will get a promt like this one:\\ </p>"},{"location":"week_1/2-terraform/#create-new-infra","title":"Create new infra","text":"<p>This command applies changes to the cloud. <pre><code>terraform apply -var=\"project=&lt;your-gcp-project-id&gt;\"\n</code></pre> After a correct application, you can go to the management platform and see your newly created resources:</p> <p></p> <p></p>"},{"location":"week_1/2-terraform/#delete-infra-after-your-work-to-avoid-costs-on-any-running-services","title":"Delete infra after your work, to avoid costs on any running services","text":"<pre><code>terraform destroy\n</code></pre>"},{"location":"week_1/3-docker/","title":"Docker","text":"<p>The course's page is here.</p> <p>We use docker to create the containers that will have the components for our application inside.</p> <pre><code>flowchart LR;\n  subgraph Docker container\n  B[Data Pipeline]\n  end\n\n  subgraph Docker container\n  C[(Table in Postgres)]\n  end\n\n  A[/Source/] ---&gt; B\n  B ---&gt; C\n  A1[/Source/] ---&gt; B\n  A2[/Source/] ---&gt; B</code></pre>"},{"location":"week_1/3-docker/#installation-and-basic-commands","title":"Installation and basic commands","text":"<p>First we install docker for our platform: https://docs.docker.com/get-docker/</p> <p>You can test the installation by running:</p> <p><pre><code>docker run hello-world\n</code></pre> Which will print the following:</p> <p></p> <p>Tip</p> <p>If you get \"<code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.</code>\" as an error, you need to start the daemon, on mac it is done by running the docker app. </p> <p>If we run the command shown in the <code>hello-world</code> image: <pre><code>docker run -it ubuntu bash\n</code></pre> We will get a container with ubuntu on it, in which we can run normal bash commands. </p> <p>Info</p> <p><code>-it</code> -&gt; interactive + terminal. When you docker run with this command it takes you straight inside the container.\\ Other common flag is <code>-d</code> -&gt; detach, wich will keep the terminal free and the machine running in the background.</p> <p>With <code>docker ps</code> or <code>docker container ls</code> we can check the containers currently running.</p> <p></p> <p>We can specify the tag(specific version) of a container that we want to use. <pre><code>docker run -it python:3.9\n</code></pre> This image will run an interactive python shell, but we will not be able to use the shell to install packages from there. </p> <p>We can specify an entrypoint to the container, for example, bash.</p> <p><pre><code>docker run -it --entrypoint=bash python:3.9\n</code></pre> We will now be in a shell able to install packages like <code>numpy</code> on our container.</p> <p></p> <p>Warning</p> <p>Remember that packages installed will not be mantained between different runs of the container, they will mantain the structure they had when downloaded.</p>"},{"location":"week_1/3-docker/#dockerfiles","title":"Dockerfiles","text":"<p>In these files we can specify instructions in order to run the containers. </p> <p>These are usually called just <code>Dockerfile</code> but you can specify names like <code>dev.Dockerfile</code>, <code>prod.Dockerfile</code> ... etc</p> <pre><code># This needs to be the first line\n# and specifies the base image.\nFROM python:3.9\n\n# We can run shell commands.\nRUN pip install pandas\n\n# We can specify the entrypoint \n# in here instead of in the CLI.\nENTRYPOINT [ \"bash\" ]\n</code></pre> <p>Now we can build an image based on <code>python:3.9</code> that also uses performs these instructions.</p> <p><pre><code>docker build -t test:pandas ./week1/docker/test\n</code></pre> We specify the <code>name:tag</code> of the container with <code>-t</code> and after that the path where we will take the config for the image (This folder is the one to contain the Dockerfile)</p> <p></p> <p>We can also copy files from our system to the image adding:</p> <pre><code># We first select the working directory\nWORKDIR /app\n# We copy the file with the structure:\n# COPY file_in_system file_in_image\nCOPY pipeline.py pipeline.py\n</code></pre> <p>we can run this image by using: <pre><code>docker run -it test:pandas\n</code></pre></p> <p></p> <p>We can also specify that we want to execute the file on startup by changing the entrypoint: <pre><code>ENTRYPOINT [ \"python\", \"pipeline.py\"]\n</code></pre></p> <p>Another option is to add parameters to the script: <pre><code>import sys\n\nimport pandas as pd\n\nprint(sys.argv)\n\nday = sys.argv[1]\n\n# some fancy stuff with pandas\n\nprint(f'job finished successfully for day = {day}')\n</code></pre> And we can run it with: <pre><code>docker run -it test:pandas 20-02-2003\n</code></pre></p> <p></p>"},{"location":"week_1/4-postgres/","title":"Postgres","text":"<p>To use postgres we are going to run an docker image that already contains it.</p> <ul> <li>We will use the <code>-e</code> tag to add environmental variables.</li> <li>The <code>-v</code> tag will map a folder in our host into a folder in the container; any modifications in one will happen on the other. THIS FOLDER NEEDS TO BE EMPTY.</li> <li>THe <code>-p</code> tag will map a port from the host to the container.</li> </ul> <pre><code>docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/code/week_1/docker/postgres/data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\n</code></pre> <p>In this case, if you receive an error like <code>initdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty. If you want to create a new database system, either remove or empty the directory \"/var/lib/postgresql/data\" or run initdb with an argument other than \"/var/lib/postgresql/data\".</code> The host folder you are providing is not empty.</p> <p>This will create the folder structure needed for postgres. </p> <p>To interface with the image we use <code>pgcli</code>, a python package that we will add to poetry. <pre><code>poetry add \"psycopg2-binary\"\npoetry add pgcli\n</code></pre> Using the following command we can interface with the database: <pre><code>pgcli -h localhost -p 5432 -u root -d ny_taxi\n</code></pre> This allows us to use commands on the container. </p> <p>We will add jupyter and other packages to our environment to explore the data. <pre><code>poetry add jupyter pandas sqlalchemy\n</code></pre></p> <p>Now we will download the dataset, which is the NY taxi dataset, which is a common one for experimenting with data tools. The one used is this one.</p> <p><pre><code>curl -O -L --output-dir ./code/week_1/docker/postgres/raw https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> or <pre><code>wget -p ./code/week_1/docker/postgres/raw https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre></p> <p>We load this data into Postgres with the code inside this notebook.</p> <p>To see the data and interact with it we are going to use pgAdmin inside a docker container, wich we will be able to see in our browser at <code>http://localhost:8080/</code>.</p> <pre><code>docker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\n</code></pre>"},{"location":"week_1/5-networks/","title":"Networks","text":"<p>Right now the database and pgAdmin will not be able to see each other, we need to put them inside a common network.  <pre><code>\nflowchart\nsubgraph Network\n  subgraph Docker container\n  A[pgAdmin]\n  end\n\n  subgraph Docker container\n  B[(Postgres)]\n  end\nend\nA --- B</code></pre></p> <p>The code to create a network called <code>pg-network</code> is:</p> <p><pre><code>docker network create pg-network\n</code></pre> and adding the network as environmental variable to the other two containers:</p> <pre><code>docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/code/week_1/docker/postgres/data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --name pg-database \\\n  --network=pg-network \\\n  postgres:13\n\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  dpage/pgadmin4\n</code></pre> <p>Tip</p> <p>We gave the database a <code>--name</code> to configure the connection easier, since if you dont you need to check the name docker has given the container.</p> <p>We can now go to <code>localhost:8080</code> since in the pgadmin configuration we passed that as the port in our machine that will comunicate with the container.</p> <p>We will need to login with the credentials <code>admin@admin.com</code> and <code>root</code> which we gave pgadmin as env variables.</p> <p>Now clicking in <code>Add New Server</code>will show us the prompt to pass the database info, add a name and go to <code>Connection</code>.</p> <p></p> <p>Fill out the connection information with your containers data. </p> <p>On the browser to the left we can check the data. Go to <code>Servers &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; Tables</code> right click on it and select <code>First 100 Rows</code>.</p> <p>You can also use SQL code via the <code>Query Tool</code>, accessible on the tools menu or clicking on this symbol: </p> <p></p>"},{"location":"week_1/6-automatic-ingestion/","title":"Automatic Ingestion","text":""},{"location":"week_1/6-automatic-ingestion/#automating-the-pipeline","title":"Automating the pipeline","text":"<p>We are going to refactor the code to automate the ingestion of data into the database.</p> <p>The command to test the pipeline is: <pre><code>python ./code/week_1/docker/automatic_ingestion/data_ingest.py  \\\n  --user=root \\\n  --password=root \\\n  --host=localhost \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre></p> <p>We create the image for the container using the following command and docker file:</p> ConsoleDockerfile <pre><code>docker build -t taxi_ingest:v0.0.1 ./code/week_1/docker/automatic_ingestion\n</code></pre> <pre><code>FROM python:3.9.1\n\nRUN pip install pandas pyarrow sqlalchemy psycopg2-binary\n\nWORKDIR /app\nCOPY data_ingest.py data_ingest.py\n\nENTRYPOINT [ \"python\", \"data_ingest.py\"]\n</code></pre> <p>This allows us to run this container with the following command:</p> <pre><code>docker run -it --network=pg-network taxi_ingest:v0.0.1 \\\n  --user=root\\\n  --password=root \\\n  --host=pg-database \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> <p>Warning</p> <p>Be careful to add the network (so the containers can see each other) and change the <code>--host</code> to <code>pg-database</code> since the hostname of the container is not <code>localhost</code> (our computer) anymore but <code>pg-database</code>.</p>"},{"location":"week_1/7-docker-compose/","title":"Docker-Compose","text":"<p>This is a tool for defining and running multi-container Docker applications, With it, we can start the whole application using only one file.</p> <p>We can specify every parameter of the containers in a file called <code>docker-compose.yaml</code>:</p> <p>Info</p> <ul> <li>Note that we dont need to specify the network, <code>docker-compose</code> automatically puts every service inside the same network.</li> <li>The volumes have a <code>:rw</code> at the end to give read and write permissions to the container in those volumes.</li> </ul> <pre><code>services:\npg-database:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- ../postgres/data:/var/lib/postgresql/data:rw\nports:\n- \"5432:5432\"\n\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\n</code></pre> <p>Now we can call <code>docker-compose</code> to spin up all these services: <pre><code>docker-compose -f ./code/week_1/docker/automatic_ingestion/docker-compose.yml up\n</code></pre></p> <p>Tip</p> <p>You only need the <code>-f</code> flag if your <code>docker-compose.yaml</code> is not no your current directory.</p> <p>You can start the services normally with: <pre><code>docker-compose up \n</code></pre> Also you can run them in detached mode if you use the <code>-d</code> flag.</p>"}]}