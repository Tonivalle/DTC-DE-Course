{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This repository is for the 2023 DTC Data Engineering course.</p> <p>https://github.com/DataTalksClub/data-engineering-zoomcamp</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Week 1: Introduction &amp; Prerequisites</li> <li> Week 2: Workflow Orchestration</li> <li> Week 3: Data Warehouse</li> <li> Week 4: Analytics engineering</li> <li> Week 5: Batch processing</li> <li> Week 6: Streaming</li> <li> Week 7, 8 &amp; 9: Project</li> </ul>"},{"location":"week_1/1-google-cloud/","title":"Google Cloud","text":""},{"location":"week_1/1-google-cloud/#1-sing-up-to-google-cloud","title":"1. Sing up to Google Cloud","text":"<p>The Cloud provider used for this project will be Google Cloud.</p> <p>https://console.cloud.google.com/getting-started</p>"},{"location":"week_1/1-google-cloud/#2-install-google-cloud-cli","title":"2. Install Google Cloud CLI","text":"<p>Follow the installation guide to install the CLI https://cloud.google.com/sdk/docs/install</p> <p>The CLI commands are here.</p>"},{"location":"week_1/1-google-cloud/#3-setup-project","title":"3. Setup project","text":""},{"location":"week_1/1-google-cloud/#set-up-adc-application-default-credentials","title":"Set up ADC (Application Default Credentials)","text":"<p>You need to set-up ADC for the environment you are going to use. To use your own account: <pre><code>gcloud auth application-default login\n</code></pre> Export the variable to the environment once you have logged in (The following path is the default for UNIX): <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.config/gcloud/application_default_credentials.json\"\n</code></pre> On this tutorial we will use a service account.</p> <p>To create a service account, you have to got to:</p> <p></p> <p>In there, we create a new service account with Storage Admin + Storage Object Admin + BigQuery Admin permissions.</p> <p>We generate a key for that account by clicking the 3 dots and adding a key.</p> <p></p> <p>Once it downloads, you can export it the same as your personal account and log in using that account. <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"&lt;path/to/your/service-account-authkeys&gt;.json\"\n\ngcloud auth activate-service-account --key-file \"&lt;path/to/your/service-account-authkeys&gt;.json\"\n</code></pre> You should now see the account if you do: <pre><code>gcloud auth list\n</code></pre> and can now activate that account with: <pre><code>gcloud config set account &lt;accountemailaddress&gt;\n</code></pre></p> <p>For more info you can check the Tutorial.</p> <p>Enable these APIs for your project (by clicking on the links):</p> <p>https://console.cloud.google.com/apis/library/iam.googleapis.com https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com https://console.cloud.google.com/apis/library/bigquery.googleapis.com</p> <p>Once we do that we can install python client libraries. e.g.: <pre><code>poetry add google-cloud-storage\n</code></pre> or  <pre><code>pip install --upgrade google-cloud-storage\n</code></pre></p>"},{"location":"week_1/2-terraform/","title":"Terraform","text":""},{"location":"week_1/2-terraform/#1-install-terraform","title":"1. Install Terraform","text":"<p>Terraform is an open-source infrastructure as code software tool. It will be used to generate the GCP infrastructure. <pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre></p>"},{"location":"week_1/2-terraform/#other-options","title":"Other options","text":"<p>https://developer.hashicorp.com/terraform/downloads</p>"},{"location":"week_1/2-terraform/#terraform-gcp-get-started-guide","title":"Terraform GCP get started guide","text":"<p>https://developer.hashicorp.com/terraform/tutorials/gcp-get-started</p>"},{"location":"week_1/2-terraform/#2-code-your-infrastructure-with-terraform","title":"2. Code your infrastructure with Terraform","text":""},{"location":"week_1/2-terraform/#maintf","title":"main.tf","text":"<p>This file contains these 4 basic declarations: * <code>terraform</code>: configure basic Terraform settings to provision your infrastructure    * <code>required_version</code>: minimum Terraform version to apply to your configuration    * <code>backend</code>: stores Terraform's \"state\" snapshots, to map real-world resources to your configuration.       * <code>local</code>: stores state file locally as <code>terraform.tfstate</code>    * <code>required_providers</code>: specifies the providers required by the current module * <code>provider</code>:    * adds a set of resource types and/or data sources that Terraform can manage    * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms. * <code>resource</code>   * Physical component and its attributes.   * blocks to define components of your infrastructure   * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table</p> <p>Note</p> <p>The resources change name with the different providers; in this case the required_providers source is <code>hashicorp/google</code> so we get <code>google_storage_bucket</code>for example. If it was <code>aws</code> we would have to use the <code>s3</code> version of the package.</p>"},{"location":"week_1/2-terraform/#variablestf","title":"variables.tf","text":"<ul> <li><code>variable</code> &amp; <code>locals</code> are runtime arguments and constants</li> </ul> <p>They are used in the <code>main.tf</code> like so:</p> <p>variables.tf <pre><code>variable \"project\" {\ndescription = \"Your GCP Project ID\"\n}\n\nvariable \"region\" {\ndescription = \"Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations\"\ndefault = \"europe-west1\"\ntype = string\n}\n</code></pre> main.tf: <pre><code>provider \"google\" {\nproject = var.project\nregion = var.region\n}\n</code></pre></p>"},{"location":"week_1/2-terraform/#3-generate-your-infrastructure-via-terraform","title":"3. Generate your infrastructure via Terraform","text":"<p>Terraform has few associated commands  that we need to use:</p>"},{"location":"week_1/2-terraform/#initialize-state-file-tfstate","title":"Initialize state file (.tfstate)","text":"<p><pre><code>terraform init\n</code></pre> </p>"},{"location":"week_1/2-terraform/#check-changes-to-new-infra-plan","title":"Check changes to new infra plan","text":"<p>This command matches changes against the previous state. <pre><code>terraform plan -var=\"project=&lt;your-gcp-project-id&gt;\"\n</code></pre> If you dont add the <code>-var</code> argument, You will get a promt like this one:\\ </p>"},{"location":"week_1/2-terraform/#create-new-infra","title":"Create new infra","text":"<p>This command applies changes to the cloud. <pre><code>terraform apply -var=\"project=&lt;your-gcp-project-id&gt;\"\n</code></pre> After a correct application, you can go to the management platform and see your newly created resources:</p> <p></p> <p></p>"},{"location":"week_1/2-terraform/#delete-infra-after-your-work-to-avoid-costs-on-any-running-services","title":"Delete infra after your work, to avoid costs on any running services","text":"<pre><code>terraform destroy\n</code></pre>"},{"location":"week_1/3-docker/","title":"Docker","text":"<p>The course's page is here.</p> <p>We use docker to create the containers that will have the components for our application inside.</p> <pre><code>flowchart LR;\n  subgraph Docker container\n  B[Data Pipeline]\n  end\n\n  subgraph Docker container\n  C[(Table in Postgres)]\n  end\n\n  A[/Source/] ---&gt; B\n  B ---&gt; C\n  A1[/Source/] ---&gt; B\n  A2[/Source/] ---&gt; B</code></pre>"},{"location":"week_1/3-docker/#installation-and-basic-commands","title":"Installation and basic commands","text":"<p>First we install docker for our platform: https://docs.docker.com/get-docker/</p> <p>You can test the installation by running:</p> <p><pre><code>docker run hello-world\n</code></pre> Which will print the following:</p> <p></p> <p>Tip</p> <p>If you get \"<code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.</code>\" as an error, you need to start the daemon, on mac it is done by running the docker app. </p> <p>If we run the command shown in the <code>hello-world</code> image: <pre><code>docker run -it ubuntu bash\n</code></pre> We will get a container with ubuntu on it, in which we can run normal bash commands. </p> <p>Info</p> <p><code>-it</code> -&gt; interactive + terminal. When you docker run with this command it takes you straight inside the container.\\ Other common flag is <code>-d</code> -&gt; detach, wich will keep the terminal free and the machine running in the background.</p> <p>With <code>docker ps</code> or <code>docker container ls</code> we can check the containers currently running.</p> <p></p> <p>We can specify the tag(specific version) of a container that we want to use. <pre><code>docker run -it python:3.9\n</code></pre> This image will run an interactive python shell, but we will not be able to use the shell to install packages from there. </p> <p>We can specify an entrypoint to the container, for example, bash.</p> <p><pre><code>docker run -it --entrypoint=bash python:3.9\n</code></pre> We will now be in a shell able to install packages like <code>numpy</code> on our container.</p> <p></p> <p>Warning</p> <p>Remember that packages installed will not be mantained between different runs of the container, they will mantain the structure they had when downloaded.</p>"},{"location":"week_1/3-docker/#dockerfiles","title":"Dockerfiles","text":"<p>In these files we can specify instructions in order to run the containers. </p> <p>These are usually called just <code>Dockerfile</code> but you can specify names like <code>dev.Dockerfile</code>, <code>prod.Dockerfile</code> ... etc</p> <pre><code># This needs to be the first line\n# and specifies the base image.\nFROM python:3.9\n\n# We can run shell commands.\nRUN pip install pandas\n\n# We can specify the entrypoint \n# in here instead of in the CLI.\nENTRYPOINT [ \"bash\" ]\n</code></pre> <p>Now we can build an image based on <code>python:3.9</code> that also uses performs these instructions.</p> <p><pre><code>docker build -t test:pandas ./week1/docker/test\n</code></pre> We specify the <code>name:tag</code> of the container with <code>-t</code> and after that the path where we will take the config for the image (This folder is the one to contain the Dockerfile)</p> <p></p> <p>We can also copy files from our system to the image adding:</p> <pre><code># We first select the working directory\nWORKDIR /app\n# We copy the file with the structure:\n# COPY file_in_system file_in_image\nCOPY pipeline.py pipeline.py\n</code></pre> <p>we can run this image by using: <pre><code>docker run -it test:pandas\n</code></pre></p> <p></p> <p>We can also specify that we want to execute the file on startup by changing the entrypoint: <pre><code>ENTRYPOINT [ \"python\", \"pipeline.py\"]\n</code></pre></p> <p>Another option is to add parameters to the script: <pre><code>import sys\n\nimport pandas as pd\n\nprint(sys.argv)\n\nday = sys.argv[1]\n\n# some fancy stuff with pandas\n\nprint(f'job finished successfully for day = {day}')\n</code></pre> And we can run it with: <pre><code>docker run -it test:pandas 20-02-2003\n</code></pre></p> <p></p>"},{"location":"week_1/3-docker/#docker-hub","title":"Docker hub","text":"<p>Docker hub is a service that can host your images. To use it you need to create an account and then do:</p> <pre><code># First we login on docker\ndocker login\n\n# Now we push the image\ndocker image push &lt;username&gt;/&lt;repository&gt;:&lt;tag&gt;\n#For example:\ndocker image push tonivalle/prefect:etl_deploy\n</code></pre>"},{"location":"week_1/4-postgres/","title":"Postgres","text":"<p>To use postgres we are going to run an docker image that already contains it.</p> <ul> <li>We will use the <code>-e</code> tag to add environmental variables.</li> <li>The <code>-v</code> tag will map a folder in our host into a folder in the container; any modifications in one will happen on the other. THIS FOLDER NEEDS TO BE EMPTY.</li> <li>THe <code>-p</code> tag will map a port from the host to the container.</li> </ul> <pre><code>docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/code/week_1/docker/postgres/data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\n</code></pre> <p>In this case, if you receive an error like <code>initdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty. If you want to create a new database system, either remove or empty the directory \"/var/lib/postgresql/data\" or run initdb with an argument other than \"/var/lib/postgresql/data\".</code> The host folder you are providing is not empty.</p> <p>This will create the folder structure needed for postgres. </p> <p>To interface with the image we use <code>pgcli</code>, a python package that we will add to poetry. <pre><code>poetry add \"psycopg2-binary\"\npoetry add pgcli\n</code></pre> Using the following command we can interface with the database: <pre><code>pgcli -h localhost -p 5432 -u root -d ny_taxi\n</code></pre> This allows us to use commands on the container. </p> <p>We will add jupyter and other packages to our environment to explore the data. <pre><code>poetry add jupyter pandas sqlalchemy\n</code></pre></p> <p>Now we will download the dataset, which is the NY taxi dataset, which is a common one for experimenting with data tools. The one used is this one.</p> <p><pre><code>curl -O -L --output-dir ./src/dtc_de_course/week_1/docker/postgres/raw https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> or <pre><code>wget -p ./src/dtc_de_course/week_1/docker/postgres/raw https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre></p> <p>We load this data into Postgres with the code inside this notebook.</p> <p>To see the data and interact with it we are going to use pgAdmin inside a docker container, wich we will be able to see in our browser at <code>http://localhost:8080/</code>.</p> <pre><code>docker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\n</code></pre>"},{"location":"week_1/5-networks/","title":"Networks","text":"<p>Right now the database and pgAdmin will not be able to see each other, we need to put them inside a common network.  <pre><code>\nflowchart\nsubgraph Network\n  subgraph Docker container\n  A[pgAdmin]\n  end\n\n  subgraph Docker container\n  B[(Postgres)]\n  end\nend\nA --- B</code></pre></p> <p>The code to create a network called <code>pg-network</code> is:</p> <p><pre><code>docker network create pg-network\n</code></pre> and adding the network as environmental variable to the other two containers:</p> <pre><code>docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/code/week_1/docker/postgres/data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --name pg-database \\\n  --network=pg-network \\\n  postgres:13\n\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  dpage/pgadmin4\n</code></pre> <p>Tip</p> <p>We gave the database a <code>--name</code> to configure the connection easier, since if you dont you need to check the name docker has given the container.</p> <p>We can now go to <code>localhost:8080</code> since in the pgadmin configuration we passed that as the port in our machine that will comunicate with the container.</p> <p>We will need to login with the credentials <code>admin@admin.com</code> and <code>root</code> which we gave pgadmin as env variables.</p> <p>Now clicking in <code>Add New Server</code>will show us the prompt to pass the database info, add a name and go to <code>Connection</code>.</p> <p></p> <p>Fill out the connection information with your containers data. </p> <p>On the browser to the left we can check the data. Go to <code>Servers &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; Tables</code> right click on it and select <code>First 100 Rows</code>.</p> <p>You can also use SQL code via the <code>Query Tool</code>, accessible on the tools menu or clicking on this symbol: </p> <p></p>"},{"location":"week_1/6-automatic-ingestion/","title":"Automatic Ingestion","text":""},{"location":"week_1/6-automatic-ingestion/#automating-the-pipeline","title":"Automating the pipeline","text":"<p>We are going to refactor the code to automate the ingestion of data into the database.</p> <p>The command to test the pipeline is: <pre><code>python ./src/dtc_de_course/week_1/docker/automatic_ingestion/data_ingest.py  \\\n  --user=root \\\n  --password=root \\\n  --host=localhost \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre></p> <p>We create the image for the container using the following command and docker file:</p> ConsoleDockerfile <pre><code>docker build -t taxi_ingest:v0.0.1 ./src/dtc_de_course/week_1/docker/automatic_ingestion\n</code></pre> <pre><code>FROM python:3.9.1\n\nRUN pip install pandas pyarrow sqlalchemy psycopg2-binary\n\nWORKDIR /app\nCOPY data_ingest.py data_ingest.py\n\nENTRYPOINT [ \"python\", \"data_ingest.py\"]\n</code></pre> <p>This allows us to run this container with the following command:</p> <pre><code>docker run -it --network=pg-network taxi_ingest:v0.0.1 \\\n  --user=root\\\n  --password=root \\\n  --host=pg-database \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> <p>Warning</p> <p>Be careful to add the network (so the containers can see each other) and change the <code>--host</code> to <code>pg-database</code> since the hostname of the container is not <code>localhost</code> (our computer) anymore but <code>pg-database</code>.</p>"},{"location":"week_1/7-docker-compose/","title":"Docker-Compose","text":"<p>This is a tool for defining and running multi-container Docker applications, With it, we can start the whole application using only one file.</p> <p>We can specify every parameter of the containers in a file called <code>docker-compose.yaml</code>:</p> <p>Info</p> <ul> <li>Note that we dont need to specify the network, <code>docker-compose</code> automatically puts every service inside the same network.</li> <li>The volumes have a <code>:rw</code> at the end to give read and write permissions to the container in those volumes.</li> </ul> <pre><code>services:\npg-database:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- ../postgres/data:/var/lib/postgresql/data:rw\nports:\n- \"5432:5432\"\n\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\n</code></pre> <p>Now we can call <code>docker-compose</code> to spin up all these services: <pre><code>docker-compose -f ./src/dtc_de_course/week_1/docker/automatic_ingestion/docker-compose.yml up\n</code></pre></p> <p></p> <p>Tip</p> <p>You only need the <code>-f</code> flag if your <code>docker-compose.yaml</code> is not no your current directory.</p> <p>You can start the services normally with: <pre><code>docker-compose up \n</code></pre> Also you can run them in detached mode if you use the <code>-d</code> flag.</p>"},{"location":"week_1/8-google-cloud-env/","title":"Setting up the Environment on Google Cloud (Cloud VM + SSH access)","text":"<p>We will need a VM instance in Google Cloud that contains everything we need for the project. </p>"},{"location":"week_1/8-google-cloud-env/#create-a-vm-instance","title":"Create a VM Instance","text":"<p>First we generate an ssh key to connect following the documentation.</p> <p>The command for MacOS is: <pre><code>ssh-keygen -t rsa -f ~/.ssh/gcp -C tonivalle -b 2048\n</code></pre> This will generate two ssh keys, one public and on private, inside <code>~/.ssh</code>. Fore more info on ssh keys, click here.</p> <p>Go to <code>Menu Bar &gt; Compute Engine &gt; Metadata</code>:</p> <p></p> <p>Info</p> <p>You may need to enable the Compute Engine API (<code>Menu Bar &gt; Compute Engine</code>) to be allowed to add the ssh key and otherwise use this functionality.</p> <p>In the <code>ssh keys</code> tab you will need to add your public key. All the VM instances you create will inherit these keys. To see the ssh key you can use  <pre><code>cat ~/.ssh/gcp.pub\n</code></pre></p> <p>Go to <code>Menu Bar &gt; Compute Engine &gt; VM Instances</code>:</p> <p></p> <p>You can click on <code>Create Instance</code>and add the configuration depending on your location: </p> <p>Danger</p> <p>We are using a machine of type <code>e2-standard-4 (4 vCPU, 16 GB memory)</code> which costs $0.16 per hour. It is important to shut down the VM when we are not working to not pay more than it is needed.</p> <p>On boot disk we will use Ubuntu 20.04 with 30 GB of memory </p> <p>Now we can connect to the VM via: <pre><code>ssh -i ~/.ssh/gcp &lt;Username inside your key&gt;@&lt;Public IP of your VM&gt;\n</code></pre></p> <p>Another way to connect is to use a ssh config file:</p> Create the ssh configFill the config infoConnect to the VM <pre><code>touch  ~/.ssh/config\ncode  ~/.ssh/config\n</code></pre> <pre><code>Host &lt;Name of your VM&gt;\nHostName &lt;Public IP of your VM&gt;\nUser &lt;Username inside your key&gt;\nIdentityFile ~/.ssh/gcp\n</code></pre> <pre><code>ssh &lt;Name of your VM&gt;\n</code></pre>"},{"location":"week_1/8-google-cloud-env/#work-with-the-vm-inside-vscode","title":"Work with the VM inside VSCode","text":"<p>To allow editing files via ssh we can install the VSCode <code>Remote - SSH</code> Extension. Now, if we click on this button on the bottom left:</p> <p></p> <p>We can use ssh inside VSCode.</p> <p>On the VM we will use:</p>"},{"location":"week_1/8-google-cloud-env/#configure-the-environment","title":"Configure the Environment","text":"<pre><code>sudo apt update\nsudo apt install python3 python3-dev python3-venv\nsudo apt-get install wget\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\nsudo apt-get install docker.io\n</code></pre> <p>We can use docker without having to type sudo every time if we run these commands (You will need to log out of ssh and back in for the membership to aply): <pre><code>sudo groupadd docker\nsudo gpasswd -a $USER docker\nsudo service docker restart\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#pyenv","title":"Pyenv","text":"<p>To configure pyenv (not needed but always a good idea) we can do: <pre><code>curl https://pyenv.run | bash\n\nsudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n</code></pre> And add this to our .bashrc and restart the shell: <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n</code></pre> Execute now: <pre><code>pyenv install 3.9.5\npyenv global 3.9.5\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#poetry","title":"Poetry","text":"<p>We can install the package's dependencies via poetry like in our local machine: <pre><code>pip install poetry\npoetry install\npoetry shell\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#docker-compose","title":"Docker-compose","text":"<p>We need to add docker-compose to the VM. We can do that bu running: <pre><code>mkdir bin\ncd bin\nwget https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64 -O docker-compose # Change de direction to the latest release of docker-compose for your architecture.\nchmod +x docker-compose\n</code></pre> Now we add it to the .bashrc <pre><code>export PATH=\"${HOME}/bin:${PATH}\"\n</code></pre></p> <p>To execute the docker-compose we clone the repo inside our VM and execute it: <pre><code>docker-compose -f ./src/dtc_de_course/week_1/docker/automatic_ingestion/docker-compose.yml -d up\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#port-forwarding","title":"Port Forwarding","text":"<p>This technique allows us to connect ports in our VM to our local computer.</p> <p>It can be done via this command depending on the direction:</p> If your connection is already establishedRemote port to localhostLocal port to remote <pre><code># Type in the console\n~C\n-R 8080:localhost:80\n-L 17125:localhost:12125\n</code></pre> <pre><code># Establish connections with the ports already forwarded\nssh -R [REMOTE:]REMOTE_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER\n# Example\nssh -R 8080:localhost:80 public.example.com\n</code></pre> <pre><code># Establish connections with the ports already forwarded\nssh -L [LOCAL_IP:]LOCAL_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER\n# Example\nssh -L 3336:db001.host:3306 user@pub001.host\n</code></pre> <p>Alternatively, you can use VSCode to manage it for you:</p> <p></p> <p>With this configuration, we can access the database on the VM in our local command line without having to connect via ssh. <pre><code>pgcli -h 127.0.0.1 -p 5432 -U root -d ny_taxi\n</code></pre></p> <p>Note</p> <p>Notice we changed the host (<code>-h</code>) from localhost to 127.0.0.1</p> <p>Connecting to <code>http://localhost:8080/</code> lets us see pgAdmin like if it was running on our machine.</p> <p>If we also forward port <code>8888</code> we can see jupyter notebooks started on our VM in our local browser (You may need to copy the url in the VM's console since it includes the token).</p>"},{"location":"week_1/8-google-cloud-env/#ingesting-the-data","title":"Ingesting the data","text":"<p>We can run the data ingestion pipeline by using:</p> <pre><code>docker build -t taxi_ingest:v0.0.1 ./src/dtc_de_course/week_1/docker/automatic_ingestion\n\ndocker run -it --network=automatic_ingestion_default taxi_ingest:v0.0.1 \\\n   --user=root \\\n   --password=root \\\n   --host=pg-database \\\n   --port=5432 \\\n   --db=ny_taxi \\\n   --table_name=yellow_taxi_trips \\\n   --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> <p>Note</p> <p>As always, the network name needs to be the same as your other containers in the network.</p>"},{"location":"week_1/8-google-cloud-env/#adding-terraform-to-the-vm","title":"Adding terraform to the VM","text":"<p>We can add terraform to the VM: <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#adding-the-service-account-to-the-vm","title":"Adding the service account to the VM","text":"<p>We created a service account in the first section of the week's folder.</p> <p>To do this we can use SFTP (Secure File Transfer Protocol) to do it. The json file for the service account was inside <code>$HOME/.config/gcloud/</code> in our case.</p> <p><pre><code>cd $HOME/.config/gcloud/\n\n# This will open an sftp console\nsftp dte-vm\n\nmkdir .gc\ncd .gc\nput &lt;Name of your key&gt;.json\n</code></pre> With this your key should be inside your VM.</p> <p>We will need to set this google application credential since we cant OAUTH from the VM. <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/&lt;Name of your key&gt;.json\ngcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#using-terraform-on-the-vm","title":"Using terraform on the VM","text":"<p>We can do the same in here as we did on our local machine. We will need to init, plan and</p> <pre><code>terraform -chdir=./src/dtc_de_course/week_1/terraform init\nterraform -chdir=./src/dtc_de_course/week_1/terraform plan -var=\"project=vast-bounty-142716\" # This is your project id\n</code></pre>"},{"location":"week_1/8-google-cloud-env/#stop-the-vm","title":"Stop the VM","text":"<p>You can stop the VM via the UI for GCP or using  <pre><code>sudo shutdown\n</code></pre></p> <p>Tip</p> <p>Whenever you restart the machine you will need to pass the new IP given to your VM to your ~/.ssh/config.</p>"},{"location":"week_1/8-google-cloud-env/#info-on-shh-keys","title":"Info on shh keys","text":"<p>via ssh.com.</p> <p>The following simple steps are required to set up public key authentication (for SSH):</p> <ol> <li>Private key stays with the user (and only there), while the public key is sent to the server. Typically with the ssh-copy-id utility.</li> <li>Server stores the public key (and \"marks\" it as authorized).</li> <li>Server will now allow access to anyone who can prove they have the corresponding private key. </li> </ol>"},{"location":"week_2/1-data-lake/","title":"Data Lake","text":""},{"location":"week_2/1-data-lake/#what-is-a-data-lake","title":"What is a Data Lake","text":"<p>A data Lake is a storage repository that holds a vast amount of raw data in its native format, coming from many sources,  until it is needed for analytics applications. </p> <ul> <li> <p>The main goal is to ingest data as quickly as posible and making it available rapidly and efficiently.</p> </li> <li> <p>The data can be structured, semi-structured or unstructured. </p> </li> <li> <p>Usually, data is assigned METADATA for faster access.</p> </li> <li> <p>It needs to be secure, scalable and inexpensive.</p> </li> </ul>"},{"location":"week_2/1-data-lake/#data-lake-vs-data-warehouse","title":"Data Lake vs Data Warehouse","text":"Data Lakes Data Warehouses Unstructured data, in its raw form. aaa Structured, transformed data. Used for stream processing, Machine Learning and Real-time Analysis Used for Batch Processing, BI, Reporting, Data Visualization ELT (Extract, Load, Transform). In this process, the data is extracted from its source for storage in the data lake, and structured only when needed.Useful for large amounts of data. ETL (Extract, Transform, Load). In this process, data is extracted from its source(s), scrubbed, then structured so it's ready for business-end analysis.Better for smaller amounts of data. Usualy inexpensive and easier to maintain. More costly and requires more management."},{"location":"week_2/1-data-lake/#etl-vs-elt","title":"ETL vs. ELT","text":"<p>ETL usually presents more stable data at the price of higher costs. Usually done for Data Warehouses.</p> <p>ELT increases the size of data stored but the processing is done when needed and inside the storage system which lowers the number of computations performed. Prefered pipeline for Data Lakes.</p>"},{"location":"week_2/1-data-lake/#problems-of-data-lake","title":"Problems of Data Lake","text":"<p>One of the problems is the conversion of a Data Lake into a Data Swamp. It is a collection point for a lot of miscellaneous data that no longer has any sort of structure and the result of poor data management and governance.</p> <p>When there is no versioning or incompatible schemas for the same data (i.e. The same data in the same route is saved as a .pickle and then as a .parquet, making accessing it harder and harder with time and new formats.)</p> <p>If there is no METADATA, there is a risk of data becoming unusable, be it from not knowing the source, the use cases...</p> <p>Its effectiveness gets restricted if there is no way to JOIN, for the lack of a KEY element or the difference in structure or schema.</p>"},{"location":"week_2/1-data-lake/#cloud-providers-data-lakes-and-warehouses","title":"Cloud providers' Data Lakes and Warehouses","text":"AWS  GCP  Azure Data Lake S3 Google Cloud Storage Azure Blob Data Warehouse Redshift Google BigQuery Azure Synapse Analytics"},{"location":"week_2/2-workflow-orchestration/","title":"Workflow Orchestration","text":""},{"location":"week_2/2-workflow-orchestration/#what-is-workflow-orchestration","title":"What is Workflow Orchestration","text":"<p>Workflow orchestration means governing your dataflow in a way that respects the orchestration rules and your business logic. A Workflow Orchestration tool allows you to turn any code into a workflow that you can schedule, run, and observe.</p> <p>Some Workflow Orchestration tools are Apache Airflow and Prefect, which we will use in the course.</p> Example <p> Imagine that the workflow orchestration tool is your personal delivery service:</p> <ul> <li>Each order (or shopping cart) reflects your workflow, it\u2019s extremely easy and convenient to put things into a shopping cart \u2014 you just add a couple of decorators, and you\u2019re off to the races.</li> <li>Within each order (or shopping cart), you may have many products that get packaged into boxes \u2014 your tasks.</li> <li> <p>Each delivery is a workflow run.</p> </li> <li> <p>Products within the boxes may have various flavors, forms, and shapes, and they reflect what you put into your shopping cart \u2014 what you wished to be orchestrated and how.</p> </li> <li>Flavors may reflect your data replication jobs (e.g. Airbyte), data transformations (e.g. dbt), data cleaning (e.g. pandas), your ML use cases (e.g. scikit-learn), and so much more.</li> <li>Your boxes (tasks) may be as small or as big as you wish \u2014 it\u2019s your order in the end (your workflow design).</li> <li>Products inside of your boxes may come from various vendors, i.e. your data tools, e.g. dbt, Fivetran, your favorite ML frameworks, your custom data cleaning libraries...</li> <li>The delivery address may either be your home address (your data warehouse), your holiday address (your data lake), or an address of a friend (some external database, data processing service, microservice, or application).</li> </ul>"},{"location":"week_2/2-workflow-orchestration/#starting-to-use-prefect","title":"Starting to use prefect","text":"<p>To start using prefect we will need to install the libraries required to continue with the project: <pre><code>poetry add prefect prefect-sqlalchemy \"prefect-gcp[cloud_storage]\" pandas-gbq greenlet\n</code></pre></p> <p>Warning</p> <p>The <code>sqlalchemy</code> version may conflict with the requiered by <code>prefect</code>, you may need to uninstall it (It will be added lateras a dependency of <code>prefect-sqlalchemy</code>)</p>"},{"location":"week_2/2-workflow-orchestration/#transforming-our-ingestion-script-to-a-flow","title":"Transforming our ingestion script to a Flow","text":"<p>A <code>flow</code> is the most basic prefect object, it contains the workflow logic and allows you to interact and understand the state of the _workflow. They act like functions: they accept inputs, perform work, return outputs... You can turn any function into a Prefect flow by adding the <code>@flow</code> decorator.</p> <p><code>Flows</code> can call <code>tasks</code> or even other <code>flows</code> (called in this case <code>subflows</code>). A task is a function that represents a discrete unit of work in a Prefect workflow. They are not required, you can use a <code>flow</code> with functions, but you gain added functionalities like timeout, retries, traceability...</p> <p><code>Tasks</code> can also receive Metadata from upstream so you can set dependencies for tasks, like waiting for one task to en for another to begin, or not starting a task if another fails. They also allow Caching.</p> Info <p>Some useful parameters are:</p> <ul> <li>retries<ul> <li>An optional number of times to retry on task run failure.</li> </ul> </li> <li>cache_key_fn <ul> <li>An optional callable (usually <code>task_input_hash</code>) that, given the task run context and call parameters, generates a string key; if the key matches a previous completed state, that state result will be restored instead of running the task again.</li> </ul> </li> <li>cache_expiration<ul> <li>An optional amount of time represented by a <code>timedelta</code> indicating how long cached states for this task should be restorable; if not provided, cached states will never expire.    </li> </ul> </li> <li>log_prints<ul> <li>If set to True, <code>print</code> statements in the task will be redirected to the Prefect logger for the task run. Defaults to None, which indicates that the value from the flow should be used.</li> </ul> </li> </ul> Example <pre><code>    import argparse\n    import os\n    from typing import Dict\n\n    import pandas\n    import pyarrow.parquet as parquet  # type: ignore\n    from sqlalchemy import create_engine\n    from sqlalchemy.engine import Engine\n\n    from prefect import flow, task\n    from prefect.tasks import task_input_hash\n    from datetime import timedelta\n\n\n    @flow(name=\"Ingestion Flow\")\n    def pipeline():\n\n        params = _parse_params()\n\n        df, data_file_route = _get_format_data(params)\n\n        engine = _create_engine(params)\n\n        _write_to_table(df, engine)\n        _remove_raw_data_file(data_file_route)\n\n\n    @task(name=\"Parsing parameters\", cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\n    def _parse_params() -&gt; Dict:\n        parser = argparse.ArgumentParser(description=\"Ingest CSV data to Postgres\")\n\n        parser.add_argument(\"--user\", required=True, help=\"user name for postgres\")\n        parser.add_argument(\"--password\", required=True, help=\"password for postgres\")\n        parser.add_argument(\"--host\", required=True, help=\"host for postgres\")\n        parser.add_argument(\"--port\", required=True, help=\"port for postgres\")\n        parser.add_argument(\"--db\", required=True, help=\"database name for postgres\")\n        parser.add_argument(\"--url\", required=True, help=\"url of the csv file\")\n        parser.add_argument(\n            \"--table_name\",\n            required=True,\n            help=\"name of the table where we will write the results to\",\n        )\n        return vars(parser.parse_args())\n\n    @task(name=\"Obtain data\", retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\n    def _get_format_data(params:Dict) -&gt; pandas.DataFrame:\n        data_file_route = _download_data(params)\n        df = _generate_df(data_file_route)\n        return df, data_file_route\n\n\n    def _download_data(params: Dict) -&gt; str:\n        os.system(f\"curl -O -L {params['url']}\")\n        file_name = params[\"url\"].split(\"/\")[-1]\n        return f\"./{file_name}\"\n\n\n    def _generate_df(data_file_route: str) -&gt; pandas.DataFrame:\n        data = parquet.read_table(data_file_route)\n        return data.to_pandas()\n\n    @task(name=\"Create engine\")\n    def _create_engine(params: Dict) -&gt; Engine:\n        return create_engine(\n            f\"postgresql://{params['user']}:{params['password']}@{params['host']}:{params['port']}/{params['db']}\"\n        )\n\n    @task(name=\"Write to SQL\")\n    def _write_to_table(df: pandas.DataFrame, engine:Engine):\n        df.to_sql(\n            name=\"yellow_taxi_data\", con=engine, if_exists=\"replace\", chunksize=100_000\n        )\n\n    @task(name=\"Cleanup\")\n    def _remove_raw_data_file(data_file: str):\n        os.remove(data_file)\n\n\n    if __name__ == \"__main__\":\n        pipeline()\n</code></pre>"},{"location":"week_2/2-workflow-orchestration/#open-the-orion-ui-to-manage-workflows","title":"Open the Orion UI to manage Workflows","text":"<p>To start we will need to configure the API URL: <pre><code>prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n</code></pre> And now we can start the service:  <pre><code>prefect orion start\n</code></pre></p> <p>This will start the UI service in <code>http://127.0.0.1:4200/api</code> </p> <p>Here, on the left menu we can see <code>Blocks</code>, which securely store credentials and configuration to easily manage connections to external systems (like aws, gcp or kubernetes). We can set limits to concurrent tasks or get notifications.</p>"},{"location":"week_2/2-workflow-orchestration/#adding-a-block-for-sqlalchemy","title":"Adding a Block for SQLAlchemy","text":"<p>They can be installed using collections for different services (in this case, for sqlalchemy, is <code>prefect_sqlalchemy</code>).</p> <p>On the UI, we will create a block of SQLAlchemy Connector using a Sync Driver and type postgresql+psycopg2. We can also add the other information like user, password... Ending with something like this:</p> <p></p> <p>We will use the class <code>SqlAlchemyConnector</code> to give the code access to the data contained in the block. This will allow us to reduce the number of input parameters of our script since they come from the Block. In the following Example you can see the difference highlighted:</p> Not Using BlocksUsing Blocks <pre><code>    import argparse\n    import os\n    from typing import Dict\n\n    import pandas\n    import pyarrow.parquet as parquet  # type: ignore\n    from sqlalchemy import create_engine\n    from sqlalchemy.engine import Engine\n\n    from prefect import flow, task\n    from prefect.tasks import task_input_hash\n    from datetime import timedelta\n\n\n    @flow(name=\"Ingestion Flow\")\n    def pipeline():\n\n        params = _parse_params()\n\n        df, data_file_route = _get_format_data(params)\n\nengine = _create_engine(params)\n_write_to_table(df, engine)\n        _remove_raw_data_file(data_file_route)\n\n\n    @task(name=\"Parsing parameters\", cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\n    def _parse_params() -&gt; Dict:\n        parser = argparse.ArgumentParser(description=\"Ingest CSV data to Postgres\")\n\nparser.add_argument(\"--user\", required=True, help=\"user name for postgres\")\nparser.add_argument(\"--password\", required=True, help=\"password for postgres\")\nparser.add_argument(\"--host\", required=True, help=\"host for postgres\")\nparser.add_argument(\"--port\", required=True, help=\"port for postgres\")\nparser.add_argument(\"--db\", required=True, help=\"database name for postgres\")\nparser.add_argument(\"--url\", required=True, help=\"url of the csv file\")\n        parser.add_argument(\n            \"--table_name\",\n            required=True,\n            help=\"name of the table where we will write the results to\",\n        )\n        return vars(parser.parse_args())\n\n    @task(name=\"Obtain data\", retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\n    def _get_format_data(params:Dict) -&gt; pandas.DataFrame:\n        data_file_route = _download_data(params)\n        df = _generate_df(data_file_route)\n        return df, data_file_route\n\n\n    def _download_data(params: Dict) -&gt; str:\n        os.system(f\"curl -O -L {params['url']}\")\n        file_name = params[\"url\"].split(\"/\")[-1]\n        return f\"./{file_name}\"\n\n\n    def _generate_df(data_file_route: str) -&gt; pandas.DataFrame:\n        data = parquet.read_table(data_file_route)\n        return data.to_pandas()\n\n@task(name=\"Create engine\")\ndef _create_engine(params: Dict) -&gt; Engine:\nreturn create_engine(\nf\"postgresql://{params['user']}:{params['password']}@{params['host']}:{params['port']}/{params['db']}\"\n)\n@task(name=\"Write to SQL\")\n    def _write_to_table(df: pandas.DataFrame, engine:Engine):\n        df.to_sql(\n            name=\"yellow_taxi_data\", con=engine, if_exists=\"replace\", chunksize=100_000\n        )\n\n    @task(name=\"Cleanup\")\n    def _remove_raw_data_file(data_file: str):\n        os.remove(data_file)\n\n\n    if __name__ == \"__main__\":\n        pipeline()\n</code></pre> <pre><code>    import argparse\n    import os\n    from typing import Dict\n\n    import pandas\n    import pyarrow.parquet as parquet  # type: ignore\n    from sqlalchemy import create_engine\n    from sqlalchemy.engine import Engine\n\n    from prefect import flow, task\n    from prefect.tasks import task_input_hash\nfrom prefect_sqlalchemy import SqlAlchemyConnector\nfrom datetime import timedelta\n\n\n    @flow(name=\"Ingestion Flow\")\n    def pipeline():\n\n        params = _parse_params()\n\n        df, data_file_route = _get_format_data(params)\n\n        _write_to_table(df)\n\n        _remove_raw_data_file(data_file_route)\n\n\n    @task(name=\"Parsing parameters\", cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\n    def _parse_params() -&gt; Dict:\n\n        parser = argparse.ArgumentParser(description=\"Ingest CSV data to Postgres\")\n\n        parser.add_argument(\"--url\", required=True, help=\"url of the csv file\")\n        parser.add_argument(\n            \"--table_name\",\n            required=True,\n            help=\"name of the table where we will write the results to\",\n        )\n        return vars(parser.parse_args())\n\n    @task(name=\"Obtain data\", retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\n    def _get_format_data(params:Dict) -&gt; pandas.DataFrame:\n        data_file_route = _download_data(params)\n        df = _generate_df(data_file_route)\n        return df, data_file_route\n\n\n    def _download_data(params: Dict) -&gt; str:\n        os.system(f\"curl -O -L {params['url']}\")\n        file_name = params[\"url\"].split(\"/\")[-1]\n        return f\"./{file_name}\"\n\n\n    def _generate_df(data_file_route: str) -&gt; pandas.DataFrame:\n        data = parquet.read_table(data_file_route)\n        return data.to_pandas()\n\n    @task(name=\"Create engine\")\n    def _create_engine(params: Dict) -&gt; Engine:\n        return create_engine(\n            f\"postgresql://{params['user']}:{params['password']}@{params['host']}:{params['port']}/{params['db']}\"\n        )\n\n    @task(name=\"Write to SQL\")\n    def _write_to_table(df: pandas.DataFrame):\nconnection_block = SqlAlchemyConnector.load(\"postgres-connector\")\nwith connection_block.get_connection(begin=False) as engine:\ndf.to_sql(\n                name=\"yellow_taxi_data\", con=engine, if_exists=\"replace\", chunksize=100_000\n            )\n\n    @task(name=\"Cleanup\")\n    def _remove_raw_data_file(data_file: str):\n        os.remove(data_file)\n\n\n    if __name__ == \"__main__\":\n        pipeline()\n</code></pre> <p>And the call from the command line:</p> Not Using BlocksUsing Blocks <pre><code>python ./src/dtc_de_course/week_2/start/data_ingest.py   --user=root \\\n--password=root \\\n--host=localhost \\\n--port=5432 \\\n--db=ny_taxi \\\n--table_name=yellow_taxi_trips \\\n--url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> <pre><code>python ./src/dtc_de_course/week_2/start/data_ingest.py \\\n--table_name=yellow_taxi_trips \\\n--url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre>"},{"location":"week_2/3-etl-gcp-prefect/","title":"ETL with GCP &amp; Prefect","text":""},{"location":"week_2/3-etl-gcp-prefect/#introduction","title":"Introduction","text":"<p>We are going to create an ETL pipeline to automatically ingest our data into Google Cloud Storage.</p> <p>We need to already have the infrastructure up to perform the operations.</p>"},{"location":"week_2/3-etl-gcp-prefect/#gcs-bucket","title":"GCS Bucket","text":"<p>We start the prefect UI with <code>prefect orion start</code>. Here, we can create a <code>GCS Bucket</code> block. before we create we register the kind of block by: <pre><code>prefect block register -m prefect_gcp\n</code></pre></p>"},{"location":"week_2/3-etl-gcp-prefect/#gcp-credentials-block","title":"GCP credentials block","text":"<p>To use the service account we generated in week_1 we can use a <code>GCP Credentials Block</code> to store the credential info.</p> <p></p>"},{"location":"week_2/3-etl-gcp-prefect/#cgs-bucket-block","title":"CGS Bucket block","text":"<p>We can put the credentials block we just created in a GCS Bucket block:</p> <p></p>"},{"location":"week_2/3-etl-gcp-prefect/#coding-the-pipeline","title":"Coding the pipeline","text":"<p>Now we can put these blocks into our code! We will refactor the pipeline to use these classes. Right now we arent transforming the data after ingestion, but in the future we can add functions before the loading!</p> Example <pre><code>from datetime import timedelta\nfrom pathlib import Path\n\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect.tasks import task_input_hash\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n\n@flow()\ndef etl_web_to_gcs() -&gt; None:\n\"\"\"Main ETL Function\"\"\"\n    COLOR = \"yellow\"\n    YEAR = 2022\n    MONTH = 1\n\n    dataset_file = f\"{COLOR}_tripdata_{YEAR}-{MONTH:02}.parquet\"\n    dataset_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{dataset_file}\"\n\n    dataset = get_data(dataset_url)\n    data_file_path = write_local(df=dataset, color=COLOR, dataset_file=dataset_file)\n    write_gcs(path=data_file_path)\n\n\n@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\ndef get_data(url: str) -&gt; pd.DataFrame:\n    data = pd.read_parquet(url)\n    return data\n\n\n@task()\ndef write_local(df: pd.DataFrame, color: str, dataset_file: str) -&gt; Path:\n\"\"\"Write DataFrame out locally as parquet file\"\"\"\n    path = Path(f\"data/{color}/{dataset_file}\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_parquet(path, compression=\"gzip\")\n    return path\n\n\n@task\ndef write_gcs(path: Path) -&gt; None:\n\"\"\"Upload local parquet file to GCS\"\"\"\n    gcs_block = GcsBucket.load(\"dte-bucket-block\")\n    gcs_block.upload_from_path(from_path=path, to_path=path)\n\n\nif __name__ == \"__main__\":\n    etl_web_to_gcs()\n</code></pre>"},{"location":"week_2/3-etl-gcp-prefect/#gcs-to-bigquery","title":"GCS to BigQuery","text":"<p>Weve got the data in its raw form on a Bucket, we are going to add it to our BigQuery Database. Since we are simulating a usecase, we are going to download the data from GCS even if it is already in our system from the previous step.</p> <p>We created the <code>trips_data_all</code> dataset using terraform but if you dont have one, create one using terraform or the UI.</p> <p>We can load the data to BigQuery following this example:</p> Example <pre><code>from pathlib import Path\n\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n\n@flow()\ndef etl_gcs_to_bq():\n\"\"\"Main ETL flow to load data into Big Query\"\"\"\n    COLOR = \"yellow\"\n    YEAR = 2022\n    MONTH = 1\n\n    path = extract_from_gcs(COLOR, YEAR, MONTH)\n    df = transform_data(path)\n    write_to_bq(df)\n\n\n@task(retries=3)\ndef extract_from_gcs(color: str, year: int, month: int) -&gt; Path:\n\"\"\"Extract data from GCS\"\"\"\n    dataset_file = f\"{color}_tripdata_{year}-{month:02}.parquet\"\n    gcs_path = Path(f\"data/{color}/{dataset_file}\")\n\n    local_data_path = Path(__file__).parents[3]\n    gcs_block = GcsBucket.load(\"dte-bucket-block\")\n    gcs_block.get_directory(from_path=gcs_path, local_path=local_data_path)\n    return local_data_path / \"data\" / color / dataset_file\n\n\n@task(log_prints=True)\ndef transform_data(path: Path) -&gt; pd.DataFrame:\n\"\"\"Data cleaning example\"\"\"\n    df = pd.read_parquet(path)\n    print(f\"pre: missing passenger count: {df['passenger_count'].isna().sum()}\")\n    df[\"passenger_count\"].fillna(0, inplace=True)\n    print(f\"post: missing passenger count: {df['passenger_count'].isna().sum()}\")\n    return df\n\n\n@task\ndef write_to_bq(df: pd.DataFrame) -&gt; None:\n\"\"\"Write DataFrame to BiqQuery\"\"\"\n\n    gcp_credentials_block = GcpCredentials.load(\"dte-gcp-credential\")\n\ndf.to_gbq(\ndestination_table=\"trips_data_all.rides\",\nproject_id=\"vast-bounty-142716\",\ncredentials=gcp_credentials_block.get_credentials_from_service_account(),\nchunksize=500_000,\nif_exists=\"append\",\n)\nif __name__ == \"__main__\":\n    etl_gcs_to_bq()\n</code></pre> <p>Additionally, you can add data from GCS to BigQuery directly through the UI:</p> <p></p>"},{"location":"week_2/4-deployments/","title":"Deployments","text":""},{"location":"week_2/4-deployments/#parametrizing-flow-deployments","title":"Parametrizing Flow &amp; Deployments","text":"<p>Instead of hard-coding the parameters for our workflow, we can add these as arguments to our flows. This way we can have different runs that result in different data passing through the ETL pipeline.</p> Example <pre><code>from datetime import timedelta\nfrom pathlib import Path\n\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect.tasks import task_input_hash\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n\n@flow\ndef etl_multiple_dates(\nmonths: list[int] = [1, 2], year: int = 2022, color: str = \"yellow\"\n):\nfor month in months:\netl_web_to_gcs(month=month, year=year, color=color)\n@flow()\ndef etl_web_to_gcs(month: int, year: int, color: str) -&gt; None:\n\"\"\"Main ETL Function\"\"\"\n    dataset_file = f\"{color}_tripdata_{year}-{month:02}.parquet\"\n    dataset_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{dataset_file}\"\n\n    dataset = get_data(dataset_url)\n    data_file_path = write_local(df=dataset, color=color, dataset_file=dataset_file)  # type: ignore\n    write_gcs(path=data_file_path)\n\n\n@task(retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(weeks=1))\ndef get_data(url: str) -&gt; pd.DataFrame:\n    data = pd.read_parquet(url)\n    return data\n\n\n@task()\ndef write_local(df: pd.DataFrame, color: str, dataset_file: str) -&gt; Path:\n\"\"\"Write DataFrame out locally as parquet file\"\"\"\n    path = Path(f\"data/{color}/{dataset_file}\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_parquet(path, compression=\"gzip\")\n    return path\n\n\n@task\ndef write_gcs(path: Path) -&gt; None:\n\"\"\"Upload local parquet file to GCS\"\"\"\n    gcs_block = GcsBucket.load(\"dte-bucket-block\")\n    gcs_block.upload_from_path(from_path=path, to_path=path)\n\n\nif __name__ == \"__main__\":\n    etl_multiple_dates()\n</code></pre>"},{"location":"week_2/4-deployments/#deployments_1","title":"Deployments","text":"<p>A deployment allows us to trigger and schedule our flow runs via the API.</p> <p>A deployment definition is composed by packaging workflow code, settings, and infrastructure configuration so that the workflow can be managed via the Prefect API and run remotely by a Prefect agent.</p> <p>Multiple deployments can be created for a single flow, with different parameters, infrastructures, schedules... The full list of options can be found in the docs.</p> <p>For example, for our flows we can have:</p> <p><pre><code>prefect deployment build ./src/dtc_de_course/week_2/p03_param_flow/parametrized_flow.py:etl_multiple_dates -n \"Parametrized ETL\" -o ./src/dtc_de_course/week_2/p03_param_flow/parametrized_flow.yaml\n</code></pre> In this example we see that we can specify the flow that we are going to deploy using <code>:</code> after the file to specify. We also gave it the name Parametrized ETL using <code>-n</code> and the output location with <code>-o</code>.</p> <p>This file contains all the necessary data for the orchestrator to handle this deployment.</p> <p>We can also pass parameters to the flow without having to use argparse to format them using the <code>--params</code> argument:</p> <pre><code>prefect deployment build ./src/dtc_de_course/week_2/p03_param_flow/parametrized_flow.py:etl_multiple_dates \\\n-n \"Parametrized ETL\" -o ./src/dtc_de_course/week_2/p03_param_flow/parametrized_flow.yaml \\\n--params='{\"months\": [3,4,5]}'\n</code></pre> <p>To send this data to the prefect API, we use the command: <pre><code>prefect deployment apply ./src/dtc_de_course/week_2/p03_param_flow/parametrized_flow.yaml\n</code></pre> Now this deployment is visible on the <code>Deployments</code> tab inside UI.</p> <p></p> <p>In there we can edit its description, tags, parameters... like we could with the command. We can also do a custom run clicking on the 3 dots, in which we can pass different parameters from the deployment's usual ones.</p>"},{"location":"week_2/4-deployments/#work-queues-and-agents","title":"Work queues and agents","text":"<p>An agent is a very lighweight python process that lives in your execution environment (local, container, server...).</p> <p>Those agents pick their tasks from a work queue, that contains a list of deployments to run. You can have multiple work queues as well and change which one a deployment will run so you can have work queues from different environments.</p> <p>To activate an agent with a specific work queue we can do: <pre><code>prefect agent start --work-queue \"default\"\n</code></pre></p> <p></p>"},{"location":"week_2/4-deployments/#scheduling","title":"Scheduling","text":"<p>Clicking on a deployment, on the right side, you can see that there is an option to schedule. For this schedule to happen there needs to be an Agent that picks it up from its work queue.</p> <p> </p> <p>Tip</p> <p>You can also add the schedule when creating a deployment by using <code>--cron</code>, <code>--interval</code> or <code>--rrule</code>. To get a schedule of 1 minute you can do: --cron \"/1 *  * *\" / --interval 60 / --rrule 'FREQ=MINUTELY'</p> <ul> <li> <p>Cron is most appropriate for users who are already familiar with cron from previous use.</p> </li> <li> <p>Interval is best suited for deployments that need to run at some consistent cadence that isn't related to absolute time.</p> </li> <li> <p>RRule is best suited for deployments that rely on calendar logic for simple recurring schedules, irregular intervals, exclusions, or day-of-month adjustments.</p> </li> </ul>"},{"location":"week_2/5-prefect-docker/","title":"Prefect + Docker","text":""},{"location":"week_2/5-prefect-docker/#running-a-flow-in-a-docker-container","title":"Running a Flow in a Docker Container","text":"<p>To have the system be more production-ready and allow other people to access our code, we can put it on cloud storage or run it using a Docker image, with the code included in it.</p> <p>By default, prefect will look for flows inside the <code>/opt/prefect/flows</code> folder.</p> <p>We create these two files:</p> Dockerfiledocker-requirements.txt <pre><code>FROM prefecthq/prefect:2.7.7-python3.9\n\nCOPY docker-requirements.txt .\n\nRUN pip install -r docker-requirements.txt --trusted-host pypi.python.org --no-cache-dir\n\nCOPY ../flows /opt/prefect/flows RUN mkdir -p /opt/prefect/data/yellow\n</code></pre> <pre><code>pandas==1.5.2\nprefect-gcp[cloud_storage]==0.2.4\nprotobuf==4.21.11\npyarrow==10.0.1\npandas-gbq==0.18.1\n</code></pre> <p>And now we can build the image as usual by executing: <pre><code>docker build -t tonivalle/prefect:etl_deploy ./src/dtc_de_course/week_2/p04_prefect_and_docker\n</code></pre></p> <p>To push this image to Docker hub we have to do:</p> <pre><code># First we login on docker\ndocker login\n\n# Now we push the image\ndocker image push tonivalle/prefect:etl_deploy\n</code></pre>"},{"location":"week_2/5-prefect-docker/#adding-the-docker-block","title":"Adding the Docker Block","text":"<p>On the UI we will need to create a Docker Block that references our image.</p> <p></p> <p>Tip</p> <p>Alternatively, you can create this block via code: <pre><code>from prefect.infrastructure.docker import DockerContainer\n\n# alternative to creating DockerContainer block in the UI\ndocker_block = DockerContainer(\n    image=\"tonivalle/prefect:etl_deploy\",  # insert your image here\n    image_pull_policy=\"ALWAYS\",\n    auto_remove=True,\n)\n\ndocker_block.save(\"etl-test-container\", overwrite=True)\n</code></pre></p> <p>We can now create a deployment using this block. Before we did it via the CLI, but we can also do it using Python code: <pre><code>from prefect.deployments import Deployment\nfrom prefect.infrastructure.docker import DockerContainer\nfrom parametrized_flow import etl_multiple_dates\n\ndocker_block = DockerContainer.load(\"etl-test-container\")\n\ndocker_deployment = Deployment.build_from_flow(\n    flow=etl_multiple_dates,\n    name=\"docker-etl-flow\",\n    infrastructure=docker_block\n)\n\nif __name__ == \"__main__\":\n    docker_deployment.apply()\n</code></pre> And calling ( It's important to do it from the folder that contains this file): <pre><code>python docker_deploy.py\n</code></pre></p> <p>We should be able to see this new deployment inside our UI.</p>"},{"location":"week_2/5-prefect-docker/#prefect-profile","title":"Prefect profile","text":"<p>Typing: <pre><code>\u276f prefect profile ls\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Available Profiles: \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502           * default \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> We can see the available profiles. We are going to activate an agent to look for work in the default queue: <pre><code>\u276f prefect agent start -q default\n\n    Starting v2.8.0 agent connected to http://127.0.0.1:4200/api...\n\n    ___ ___ ___ ___ ___ ___ _____     _   ___ ___ _  _ _____\n    | _ \\ _ \\ __| __| __/ __|_   _|   /_\\ / __| __| \\| |_   _|\n    |  _/   / _|| _|| _| (__  | |    / _ \\ (_ | _|| .` | | |\n    |_| |_|_\\___|_| |___\\___| |_|   /_/ \\_\\___|___|_|\\_| |_|\n\n\n    Agent started! Looking for work from queue(s): default...\n</code></pre> Running the deployment will put it on the queue of the flow (in this case <code>default</code>): <pre><code>\u276f prefect deployment run etl-multiple-dates/docker-etl-flow -p \"months=[1,2]\"\n    Creating flow run for deployment 'etl-multiple-dates/docker-etl-flow'...\n    Created flow run 'thistle-ocelot'.\n    \u2514\u2500\u2500 UUID: f05a5c41-2e7f-4d59-ad27-5e25d10cacbf\n    \u2514\u2500\u2500 Parameters: {'months': [1, 2]}\n    \u2514\u2500\u2500 Scheduled start time: 2023-02-11 14:35:34 CET (now)\n    \u2514\u2500\u2500 URL: http://127.0.0.1:4200/flow-runs/flow-run/f05a5c41-2e7f-4d59-ad27-5e25d10cacbf\n</code></pre></p> <p>Warning</p> <p>You need to <code>cd</code> into this directory when creating the deployment and possibly the image; doing it from the ouside will mess imports if they dont have the same structure!</p>"},{"location":"week_3/1-data-warehouse-big-query/","title":"Data warehouse &amp; Big Query","text":""},{"location":"week_3/1-data-warehouse-big-query/#olap-vs-oltp","title":"OLAP vs OLTP","text":"<p>An OLAP system is designed to process large amounts of data quickly, allowing users to analyze multiple data dimensions in tandem. Teams can use this data for decision-making and problem-solving and users are usually data scientists/analysts. </p> <p>In contrast, OLTP systems are designed to handle large volumes of transactional data involving multiple users, in fast but small transactions.</p> OLAP OLTP Meaning On-Line Analytical Process On-Line Transaction Processing Purpose Plan, solve problems, support decisions, discover hidden insights Control and run essential business operations in real time Data updates Data periodically refreshed with scheduled, long-running batch jobs Short, fast updates initiated by user Database design Denormalized databases for analysis Normalized databases for efficiency Space requirements Generally large due to aggregating large datasets Generally small if historical data is archived Backup and recovery Lost data can be reloaded from OLTP database as needed in lieu of regular backups Regular backups required to ensure business continuity and meet legal and governance requirements Productivity Increases productivity of business managers, data analysts, and executives Increases productivity of end users Data view Multi-dimensional view of enterprise data Lists day-to-day business transactions User examples Knowledge workers such as data analysts, business analysts, and executives Customer-facing personnel, clerks, online shoppers"},{"location":"week_3/1-data-warehouse-big-query/#data-warehouses","title":"Data warehouses","text":"<p>A data warehouse is a OLAP solution, used primarily for data analysis and reporting. They contain Meta Data, Raw Data and Summary Data. This data comes from many different sources, that report to a staging area that is the one that writes to the warehouse.</p> <p>A warehouse can be divided in Data Marts, which are a data storage system that contains information specific to an organization's business unit, like sales, supply chain, HR, marketing...</p> <p>Data mats are usually used as interfaces for users but sometimes data warehouses are used by analysts for broader insights</p> <p></p>"},{"location":"week_3/1-data-warehouse-big-query/#big-query","title":"Big Query","text":"<p>Big query is a Serverless Data Warehouse (you dont have to manage a server or install software to make it work) that includes software and infrastructure from the get-go. It has built-in features for machine learning, geospatial analysis or BI, and maximizes flexibility by separating your storage and the computer engine that processes your data.</p> <p>Big query contains also public datasets like <code>bigquery-public-data.new_york_citibike.citibike_stations</code> which you can search using the explorer search bar and broadening search to all projects.</p> <p>It is a column oriented storage (instead of record oriented like <code>csv</code> files or databases like <code>Postgres</code>/<code>MySQL</code>) which helps provide better aggregation on columns, which is a more used operation type for analysis.</p> <p></p>"},{"location":"week_3/1-data-warehouse-big-query/#creating-an-external-table","title":"Creating an external table","text":"<p>You can create a table that is not stored on Big Query (an external table) by using: <pre><code>CREATE OR REPLACE EXTERNAL TABLE `vast-bounty-142716.trips_data_all.external_yellow_tripdata`\nOPTIONS (\nformat = 'parquet',\nuris = ['gs://dtc-de-data-lake/data/yellow/yellow_tripdata_2022-*.parquet']\n);\n</code></pre></p> <p>Note</p> <p>When creating external tables, Big Query will not be able to get the table size or the number of rows if you go to table information since it is not actually stored on the warehouse.</p>"},{"location":"week_3/1-data-warehouse-big-query/#partitions","title":"Partitions","text":"<p>We can create partitions by splitting the data using one or more columns. </p> <p>There is a limit of 4000 partitions per table. <pre><code>CREATE OR REPLACE TABLE vast-bounty-142716.trips_data_all.yellow_tripdata_partitoned\nPARTITION BY\nDATE(tpep_pickup_datetime) AS\nSELECT * FROM vast-bounty-142716.trips_data_all.external_yellow_tripdata;\n</code></pre> </p> <p>Partitions are a good idea in Cloud Computing since filtering using the partition column will need to process fewer data as it is already pre-filtered. <pre><code>-- Impact of partition\n-- Scanning 1.6GB of data\nSELECT DISTINCT(VendorID)\nFROM vast-bounty-142716.trips_data_all.yellow_tripdata_non_partitoned\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2022-06-01' AND '2022-06-30';\n\n-- Scanning ~106 MB of DATA\nSELECT DISTINCT(VendorID)\nFROM vast-bounty-142716.trips_data_all.yellow_tripdata_partitoned\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2022-06-01' AND '2022-06-30';\n</code></pre></p> <p>We can look at the partitions and their size: <pre><code>SELECT table_name, partition_id, total_rows\nFROM `nytaxi.INFORMATION_SCHEMA.PARTITIONS`\nWHERE table_name = 'yellow_tripdata_partitoned'\nORDER BY total_rows DESC;\n</code></pre></p> <p>Go to Google Docs for this feature.</p>"},{"location":"week_3/1-data-warehouse-big-query/#clusters","title":"Clusters","text":"<p>Clustering allows you to group data (it works inside partitions). </p> <p>This benefits us in two ways: the rows that share tha same cluster will be grouped when we see the table and the cost and performance will improve. Filter queries and aggregate queries will also show better performance, especially with tables &gt; 1GB of data. </p> <p>The order of the columns when generating the clusters is important, since that will be the way the rows will be ordered after the operation.</p> <p>The limit is of 4 clustering columns per table.</p> <p></p> <pre><code>CREATE OR REPLACE TABLE vast-bounty-142716.trips_data_all.yellow_tripdata_partitoned_clustered\nPARTITION BY DATE(tpep_pickup_datetime)\nCLUSTER BY VendorID AS\nSELECT * FROM vast-bounty-142716.trips_data_all.external_yellow_tripdata;\n\n-- Processing partitioned &amp; clustered data\n-- Query scans 1.1 GB\nSELECT count(*) as trips\nFROM vast-bounty-142716.trips_data_all.yellow_tripdata_partitoned\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2022-06-01' AND '2023-12-31'\nAND VendorID=1;\n\n-- Query scans 864.5 MB\nSELECT count(*) as trips\nFROM vast-bounty-142716.trips_data_all.yellow_tripdata_partitoned_clustered\nWHERE DATE(tpep_pickup_datetime) BETWEEN '2022-06-01' AND '2023-12-31'\nAND VendorID=1;\n</code></pre> <p>Go to Google Docs for this feature.</p>"},{"location":"week_3/1-data-warehouse-big-query/#cluster-vs-partition","title":"Cluster vs. Partition","text":"Clustering Partitoning Cost benefit unknown Cost known upfront You need more granularity than partitioning alone allows You need partition-level management. Your queries commonly use filters or aggregation against multiple particular columns Filter or aggregate on single column The cardinality of the number of values in a column or group of columns is large <p>You should choose Clustering over Partitioning if:</p> <ul> <li>Partitioning results in a small amount of data per partition (approximately less than 1 GB). When your granularity is high, its better to Cluster.</li> <li>Partitioning results in a large number of partitions beyond the limits on partitioned tables (4000 partitions).</li> <li>Partitioning results in your mutation operations modifying the majority of partitions in the table frequently (for example, every few minutes).</li> </ul>"},{"location":"week_3/1-data-warehouse-big-query/#automatic-reclustering","title":"Automatic reclustering","text":"<p>As data is added to a clustered table, the new data is organized into blocks, which might create new storage blocks or update existing blocks. Block optimization is required for optimal query and storage performance because new data might not be grouped with existing data that has the same cluster values.</p> <p>To maintain the performance characteristics of a clustered table:</p> <ul> <li>BigQuery performs automatic re-clustering in the background.</li> <li>For partitioned tables, clustering is maintained for data within the scope of each partition.</li> </ul>"},{"location":"week_3/1-data-warehouse-big-query/#best-practices-on-big-query","title":"Best practices on Big Query","text":"<ul> <li> <p>Cost reduction</p> <ul> <li>Avoid SELECT *</li> <li>Price your queries before running them (You can see this on the top right)</li> <li>Use clustered or partitioned tables</li> <li>Use streaming inserts with caution</li> <li>Materialize query results in stages</li> </ul> </li> <li> <p>Query Performance</p> <ul> <li>Filter on partitioned columns</li> <li>Denormalizing data<ul> <li>Use nested or repeated columns</li> </ul> </li> <li>Use external data sources appropriately (They may incur more cost)<ul> <li>Don't use it if you want high query performance</li> </ul> </li> <li>Reduce data before using a JOIN</li> <li>Do not treat WITH clauses as prepared statements</li> <li>Avoid oversharding tables</li> <li>Avoid JavaScript user-defined functions</li> <li>Use approximate aggregation functions (HyperLogLog++)</li> <li><code>Order</code> should be the last part of the query, for query operations to maximize performance</li> <li>Optimize your join patterns<ul> <li>As a best practice, place the table with the largest number of rows first, followed by the table with the fewest rows, and then place the remaining tables by decreasing size.</li> </ul> </li> </ul> </li> </ul>"},{"location":"week_3/1-data-warehouse-big-query/#other-notes","title":"Other notes","text":"<p>All the queries can be found on here.</p>"},{"location":"week_3/2-ml-big-query/","title":"ML in BigQuery","text":""},{"location":"week_3/2-ml-big-query/#introduction","title":"Introduction","text":"<p>The target for ML in BigQuery is Data Analysts or managers, as you dont need programming knowledge or to export the data to different systems.</p> <p>ML in BigQuery is free for:</p> <ul> <li>10 GB per month of data storage</li> <li>1 TB per month of queries processed</li> <li>while usingML Create model step: First 10 GB per month is free</li> </ul> <p>Then it can go for ~250$ per TB for most models.</p> <p></p> <p>The way to create a model is the following:</p> <p><pre><code>-- CREATE A ML TABLE WITH APPROPRIATE TYPE\nCREATE OR REPLACE TABLE `vast-bounty-142716.trips_data_all.yellow_tripdata_ml` (\n`passenger_count` INTEGER,\n`trip_distance` FLOAT64,\n`PULocationID` STRING,\n`DOLocationID` STRING,\n`payment_type` STRING,\n`fare_amount` FLOAT64,\n`tolls_amount` FLOAT64,\n`tip_amount` FLOAT64\n) AS (\nSELECT passenger_count, trip_distance, cast(PULocationID AS STRING), CAST(DOLocationID AS STRING),\nCAST(payment_type AS STRING), fare_amount, tolls_amount, tip_amount\nFROM `vast-bounty-142716.trips_data_all.yellow_tripdata_partitoned` WHERE fare_amount != 0\n);\n\n-- CREATE MODEL WITH DEFAULT SETTING\nCREATE OR REPLACE MODEL `vast-bounty-142716.trips_data_all.tip_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT') AS\nSELECT\n*\nFROM\n`vast-bounty-142716.trips_data_all.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n</code></pre> You can then explain and predict by using:</p> <pre><code>SELECT\n*\nFROM\nML.EXPLAIN_PREDICT(MODEL `vast-bounty-142716.trips_data_all.tip_model`,\n(\nSELECT\n*\nFROM\n`vast-bounty-142716.trips_data_all.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n), STRUCT(3 as top_k_features));\n</code></pre> <p>Google has a site for their tutorials on this.</p>"},{"location":"week_3/2-ml-big-query/#feature-preprocessing","title":"Feature Preprocessing","text":"<p>There are two ways to do feature preprocessing: Automatic and Manual.</p> <p>Automatic feature preprocessing offers Missing Data Imputation, Feature Transformations and Categorical Feature Encoding.</p> <p>Manual feature preprocessing offers  <code>ML.BUCKETIZE</code>, <code>ML.POLYNOMIAL_EXPAND</code>, <code>ML.FEATURE_CROSS</code>, <code>ML.NGRAMS</code>, <code>ML.QUANTILE_BUCKETIZE</code>, <code>ML.HASH_BUCKETIZE</code>, <code>ML.MIN_MAX_SCALER</code>, <code>ML.STANDARD_SCALER</code>, <code>ML.MAX_ABS_SCALER</code>, <code>ML.ROBUST_SCALER</code>, <code>ML.NORMALIZER</code>, <code>ML.IMPUTER</code>, <code>ML.ONE_HOT_ENCODER</code>, <code>ML.LABEL_ENCODER</code>,</p>"},{"location":"week_3/2-ml-big-query/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<p>You can also performe hyperparameter tuning on created models:</p> <pre><code>-- HYPER PARAM TUNNING\nCREATE OR REPLACE MODEL `vast-bounty-142716.trips_data_all.tip_hyperparam_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT',\nnum_trials=5,\nmax_parallel_trials=2,\nl1_reg=hparam_range(0, 20),\nl2_reg=hparam_candidates([0, 0.1, 1, 10])) AS\nSELECT\n*\nFROM\n`vast-bounty-142716.trips_data_all.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n</code></pre>"},{"location":"week_3/3-deployment-ml-big-query/","title":"BigQuery Machine Learning Deployment","text":"<p>We may want to export the ML model that we created using Big Query and deploy it in a docker container. The steps to do this are:</p> <ol> <li>We authenticate and export the model to Google Storage. <pre><code>gcloud auth login\nbq --project_id vast-bounty-142716 \\\n    extract -m trips_data_all.tip_model \\\n    gs://taxi_ml_model/tip_model\n</code></pre></li> <li>We copy the model from Google storage to our local machine. <pre><code>mkdir /tmp/model\ngsutil cp -r gs://taxi_ml_model/tip_model /tmp/model\n</code></pre></li> <li> <p>We move the model to the folder we will mount to the container. (This can be done in the same step as the one above) The 1 is the version of the model, which will be used later during the requests.  <pre><code>mkdir -p serving_dir/tip_model/1\ncp -r /tmp/model/tip_model/* serving_dir/tip_model/1\n</code></pre></p> </li> <li> <p>Using tensorflow's serving image we generate a container with the folder containing the model mounted. <pre><code>docker pull tensorflow/serving\ndocker run -p 8501:8501 --mount type=bind,source=pwd/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t tensorflow/serving &amp;\n</code></pre></p> </li> <li> <p>We can perform a POST request to the model to get a prediction with curl, Postman... on the model url, the v1 comes from the <code>1</code> subfolder created during step 3. <pre><code>curl -d '{\"instances\": [{\"passenger_count\":1, \"trip_distance\":12.2, \"PULocationID\":\"193\", \"DOLocationID\":\"264\", \"payment_type\":\"2\",\"fare_amount\":20.4,\"tolls_amount\":0.0}]}' -X POST http://localhost:8501/v1/models/tip_model:predict\n</code></pre></p> </li> <li>We can check the model in here: <pre><code>http://localhost:8501/v1/models/tip_model\n</code></pre></li> </ol>"},{"location":"week_4/1-analytics-engineering-basics/","title":"Analytics engineering","text":""},{"location":"week_4/1-analytics-engineering-basics/#what-is-analytics-engineering","title":"What is Analytics engineering?","text":"<p>Analytics engineering is the bridge between Data engineers and analysts.</p> <p></p> <p>Their usual competences are in Data Loading (see ETL vs ELT), Data Storing (using data warehouses and lakes), Data Modeling (using tools like dbt or Dataform) and Data Presentation (with Google Data Studio, Tableau, PowerBI...).</p>"},{"location":"week_4/1-analytics-engineering-basics/#dimensional-modeling","title":"Dimensional Modeling","text":"<p>Kimball\u2019s Dimensional Modeling has the objective of delivering data understandable to the business users and fast query performance. This is done by prioritising user understandability and query performance over non redundant data (3NF).</p> <p>Other approaches are Bill Inmon and Data vault.</p> <p>Dimensional Modeling is composed of 2 principal elements: Facts tables and Dimensions tables.</p> <ul> <li>Facts tables are composed of measurements, metrics or facts which corresponds to a business process (sales, orders...).</li> <li>Dimensions tables correspond to business entities and provide context to a business process (meaning they provide context to a Fact table by extension). They can be customers, product...</li> </ul>"},{"location":"week_4/1-analytics-engineering-basics/#architecture-of-dimensional-modeling","title":"Architecture of Dimensional Modeling","text":"<p>It is composed of 3 areas:</p> <ol> <li> <p>Stage Area</p> <ul> <li>Contains the raw data.</li> <li>Only accessible to people that know how to use/transform raw data.</li> </ul> </li> <li> <p>Processing Area</p> <ul> <li>Transforms from raw data to data models.</li> <li>Focused on efficiency and ensuring standards.</li> <li>Only accessible to people that know how people who transform the raw data.</li> </ul> </li> <li> <p>Presentation Area</p> <ul> <li>Final form of the data.</li> <li>Accessible by business stakeholders.</li> </ul> </li> </ol> <p>More information about Dimensional Modeling can be found here.</p>"},{"location":"week_4/2-dbt-basics/","title":"dbt basics","text":""},{"location":"week_4/2-dbt-basics/#what-is-dbt","title":"What is dbt","text":"<p>It stands for Data Build Tool and is a open-source transformation tool that allows anyone that knows SQL to deploy analytics code following software engineering best practices like modularity, portability, CI/CD, version controll, and documentation. </p> <p>Dbt Core is free for all use, but there is also dbt Cloud that adds functionality and is free only for individuals.</p> <p>It is used inside the data warehouse, so it is part of a ELT process inside a warehouse.</p> <p></p> <p>It works by adding a modeling layer that turns a table into a model, transforms the model into a derived model, and then persists the model back to the warehouse.</p> <p>Each model is a .sql file (that is basically a <code>select</code> statement that will compile later to a DDL/DML) that dbt will compile and run in our data warehouse.</p> <p>We are going to use it by developing on BigQuery using dbt Cloud and on our local Postgres DDBB using a local installation of dbt, running the models via the CLI.</p>"},{"location":"week_4/2-dbt-basics/#creating-a-new-dbt-project","title":"Creating a new dbt project","text":"<p>Depending on the environment, we will use different ways to start the project:</p>"},{"location":"week_4/2-dbt-basics/#with-the-cli","title":"With the CLI","text":"<p>We need to install dbt locally. For mac it is done via: <pre><code>brew update\nbrew tap dbt-labs/dbt\n\n# change this to your specific DB.\nbrew install dbt-postgres \n</code></pre> Now you set up <code>profiles.yml</code> depending on your DB from this page's table of contents on the left.</p> <p>Run <code>dbt init</code> in the path we want to start the project to clone the starter project.</p>"},{"location":"week_4/2-dbt-basics/#with-dbt-cloud-and-big-query","title":"With dbt Cloud and Big Query","text":"<p>You will need to create a dbt cloud account using this link and connect to your warehouse following these instructions. More detailed instructions in here.</p> <p>Note</p> <p>If you feel more comfortable developing locally you could use a local installation of dbt as well. You can follow the official dbt documentation or follow the dbt with BigQuery on Docker guide to setup dbt locally on docker. You will need to install the latest version (1.0) with the BigQuery adapter (dbt-bigquery).</p> <p>After having set up the dbt cloud credentials (repo and data warehouse) we can start the project from the web-based IDE by clicking on <code>initialize your project</code>.</p> <p>Once we do that we can go ahead and change the name of the project in the <code>dbt_project.yml</code> (It appears more than once later in the yml, change it everywhere)</p>"},{"location":"week_4/3-using-dbt/","title":"Using dbt","text":""},{"location":"week_4/3-using-dbt/#building-the-first-models","title":"Building the first models","text":""}]}