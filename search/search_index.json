{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This repository is for the 2023 DTC Data Engineering course.</p> <p>https://github.com/DataTalksClub/data-engineering-zoomcamp</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Week 1: Introduction &amp; Prerequisites</li> <li> Week 2: Workflow Orchestration</li> <li> Week 3: Data Warehouse</li> <li> Week 4: Analytics engineering</li> <li> Week 5: Batch processing</li> <li> Week 6: Streaming</li> <li> Week 7, 8 &amp; 9: Project</li> </ul>"},{"location":"week_1/1-google-cloud/","title":"Google Cloud","text":""},{"location":"week_1/1-google-cloud/#1-sing-up-to-google-cloud","title":"1. Sing up to Google Cloud","text":"<p>The Cloud provider used for this project will be Google Cloud.</p> <p>https://console.cloud.google.com/getting-started</p>"},{"location":"week_1/1-google-cloud/#2-install-google-cloud-cli","title":"2. Install Google Cloud CLI","text":"<p>Follow the installation guide to install the CLI https://cloud.google.com/sdk/docs/install</p> <p>The CLI commands are here.</p>"},{"location":"week_1/1-google-cloud/#3-setup-project","title":"3. Setup project","text":""},{"location":"week_1/1-google-cloud/#set-up-adc-application-default-credentials","title":"Set up ADC (Application Default Credentials)","text":"<p>You need to set-up ADC for the environment you are going to use. To use your own account: <pre><code>gcloud auth application-default login\n</code></pre> Export the variable to the environment once you have logged in (The following path is the default for UNIX): <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"$HOME/.config/gcloud/application_default_credentials.json\"\n</code></pre> On this tutorial we will use a service account.</p> <p>To create a service account, you have to got to:</p> <p></p> <p>In there, we create a new service account with Storage Admin + Storage Object Admin + BigQuery Admin permissions.</p> <p>We generate a key for that account by clicking the 3 dots and adding a key.</p> <p></p> <p>Once it downloads, you can export it the same as your personal account and log in using that account. <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"&lt;path/to/your/service-account-authkeys&gt;.json\"\n\ngcloud auth activate-service-account --key-file \"&lt;path/to/your/service-account-authkeys&gt;.json\"\n</code></pre> You should now see the account if you do: <pre><code>gcloud auth list\n</code></pre> and can now activate that account with: <pre><code>gcloud config set account &lt;accountemailaddress&gt;\n</code></pre></p> <p>For more info you can check the Tutorial.</p> <p>Enable these APIs for your project (by clicking on the links):</p> <p>https://console.cloud.google.com/apis/library/iam.googleapis.com https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com https://console.cloud.google.com/apis/library/bigquery.googleapis.com</p> <p>Once we do that we can install python client libraries. e.g.: <pre><code>poetry add google-cloud-storage\n</code></pre> or  <pre><code>pip install --upgrade google-cloud-storage\n</code></pre></p>"},{"location":"week_1/2-terraform/","title":"Terraform","text":""},{"location":"week_1/2-terraform/#1-install-terraform","title":"1. Install Terraform","text":"<p>Terraform is an open-source infrastructure as code software tool. It will be used to generate the GCP infrastructure. <pre><code>brew tap hashicorp/tap\nbrew install hashicorp/tap/terraform\n</code></pre></p>"},{"location":"week_1/2-terraform/#other-options","title":"Other options","text":"<p>https://developer.hashicorp.com/terraform/downloads</p>"},{"location":"week_1/2-terraform/#terraform-gcp-get-started-guide","title":"Terraform GCP get started guide","text":"<p>https://developer.hashicorp.com/terraform/tutorials/gcp-get-started</p>"},{"location":"week_1/2-terraform/#2-code-your-infrastructure-with-terraform","title":"2. Code your infrastructure with Terraform","text":""},{"location":"week_1/2-terraform/#maintf","title":"main.tf","text":"<p>This file contains these 4 basic declarations: * <code>terraform</code>: configure basic Terraform settings to provision your infrastructure    * <code>required_version</code>: minimum Terraform version to apply to your configuration    * <code>backend</code>: stores Terraform's \"state\" snapshots, to map real-world resources to your configuration.       * <code>local</code>: stores state file locally as <code>terraform.tfstate</code>    * <code>required_providers</code>: specifies the providers required by the current module * <code>provider</code>:    * adds a set of resource types and/or data sources that Terraform can manage    * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms. * <code>resource</code>   * Physical component and its attributes.   * blocks to define components of your infrastructure   * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table</p> <p>Note</p> <p>The resources change name with the different providers; in this case the required_providers source is <code>hashicorp/google</code> so we get <code>google_storage_bucket</code>for example. If it was <code>aws</code> we would have to use the <code>s3</code> version of the package.</p>"},{"location":"week_1/2-terraform/#variablestf","title":"variables.tf","text":"<ul> <li><code>variable</code> &amp; <code>locals</code> are runtime arguments and constants</li> </ul> <p>They are used in the <code>main.tf</code> like so:</p> <p>variables.tf <pre><code>variable \"project\" {\ndescription = \"Your GCP Project ID\"\n}\n\nvariable \"region\" {\ndescription = \"Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations\"\ndefault = \"europe-west6\"\ntype = string\n}\n</code></pre> main.tf: <pre><code>provider \"google\" {\nproject = var.project\nregion = var.region\n}\n</code></pre></p>"},{"location":"week_1/2-terraform/#3-generate-your-infrastructure-via-terraform","title":"3. Generate your infrastructure via Terraform","text":"<p>Terraform has few associated commands  that we need to use:</p>"},{"location":"week_1/2-terraform/#initialize-state-file-tfstate","title":"Initialize state file (.tfstate)","text":"<p><pre><code>terraform init\n</code></pre> </p>"},{"location":"week_1/2-terraform/#check-changes-to-new-infra-plan","title":"Check changes to new infra plan","text":"<p>This command matches changes against the previous state. <pre><code>terraform plan -var=\"project=&lt;your-gcp-project-id&gt;\"\n</code></pre> If you dont add the <code>-var</code> argument, You will get a promt like this one:\\ </p>"},{"location":"week_1/2-terraform/#create-new-infra","title":"Create new infra","text":"<p>This command applies changes to the cloud. <pre><code>terraform apply -var=\"project=&lt;your-gcp-project-id&gt;\"\n</code></pre> After a correct application, you can go to the management platform and see your newly created resources:</p> <p></p> <p></p>"},{"location":"week_1/2-terraform/#delete-infra-after-your-work-to-avoid-costs-on-any-running-services","title":"Delete infra after your work, to avoid costs on any running services","text":"<pre><code>terraform destroy\n</code></pre>"},{"location":"week_1/3-docker/","title":"Docker","text":"<p>The course's page is here.</p> <p>We use docker to create the containers that will have the components for our application inside.</p> <pre><code>flowchart LR;\n  subgraph Docker container\n  B[Data Pipeline]\n  end\n\n  subgraph Docker container\n  C[(Table in Postgres)]\n  end\n\n  A[/Source/] ---&gt; B\n  B ---&gt; C\n  A1[/Source/] ---&gt; B\n  A2[/Source/] ---&gt; B</code></pre>"},{"location":"week_1/3-docker/#installation-and-basic-commands","title":"Installation and basic commands","text":"<p>First we install docker for our platform: https://docs.docker.com/get-docker/</p> <p>You can test the installation by running:</p> <p><pre><code>docker run hello-world\n</code></pre> Which will print the following:</p> <p></p> <p>Tip</p> <p>If you get \"<code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.</code>\" as an error, you need to start the daemon, on mac it is done by running the docker app. </p> <p>If we run the command shown in the <code>hello-world</code> image: <pre><code>docker run -it ubuntu bash\n</code></pre> We will get a container with ubuntu on it, in which we can run normal bash commands. </p> <p>Info</p> <p><code>-it</code> -&gt; interactive + terminal. When you docker run with this command it takes you straight inside the container.\\ Other common flag is <code>-d</code> -&gt; detach, wich will keep the terminal free and the machine running in the background.</p> <p>With <code>docker ps</code> or <code>docker container ls</code> we can check the containers currently running.</p> <p></p> <p>We can specify the tag(specific version) of a container that we want to use. <pre><code>docker run -it python:3.9\n</code></pre> This image will run an interactive python shell, but we will not be able to use the shell to install packages from there. </p> <p>We can specify an entrypoint to the container, for example, bash.</p> <p><pre><code>docker run -it --entrypoint=bash python:3.9\n</code></pre> We will now be in a shell able to install packages like <code>numpy</code> on our container.</p> <p></p> <p>Warning</p> <p>Remember that packages installed will not be mantained between different runs of the container, they will mantain the structure they had when downloaded.</p>"},{"location":"week_1/3-docker/#dockerfiles","title":"Dockerfiles","text":"<p>In these files we can specify instructions in order to run the containers. </p> <p>These are usually called just <code>Dockerfile</code> but you can specify names like <code>dev.Dockerfile</code>, <code>prod.Dockerfile</code> ... etc</p> <pre><code># This needs to be the first line\n# and specifies the base image.\nFROM python:3.9\n\n# We can run shell commands.\nRUN pip install pandas\n\n# We can specify the entrypoint \n# in here instead of in the CLI.\nENTRYPOINT [ \"bash\" ]\n</code></pre> <p>Now we can build an image based on <code>python:3.9</code> that also uses performs these instructions.</p> <p><pre><code>docker build -t test:pandas ./week1/docker/test\n</code></pre> We specify the <code>name:tag</code> of the container with <code>-t</code> and after that the path where we will take the config for the image (This folder is the one to contain the Dockerfile)</p> <p></p> <p>We can also copy files from our system to the image adding:</p> <pre><code># We first select the working directory\nWORKDIR /app\n# We copy the file with the structure:\n# COPY file_in_system file_in_image\nCOPY pipeline.py pipeline.py\n</code></pre> <p>we can run this image by using: <pre><code>docker run -it test:pandas\n</code></pre></p> <p></p> <p>We can also specify that we want to execute the file on startup by changing the entrypoint: <pre><code>ENTRYPOINT [ \"python\", \"pipeline.py\"]\n</code></pre></p> <p>Another option is to add parameters to the script: <pre><code>import sys\n\nimport pandas as pd\n\nprint(sys.argv)\n\nday = sys.argv[1]\n\n# some fancy stuff with pandas\n\nprint(f'job finished successfully for day = {day}')\n</code></pre> And we can run it with: <pre><code>docker run -it test:pandas 20-02-2003\n</code></pre></p> <p></p>"},{"location":"week_1/4-postgres/","title":"Postgres","text":"<p>To use postgres we are going to run an docker image that already contains it.</p> <ul> <li>We will use the <code>-e</code> tag to add environmental variables.</li> <li>The <code>-v</code> tag will map a folder in our host into a folder in the container; any modifications in one will happen on the other. THIS FOLDER NEEDS TO BE EMPTY.</li> <li>THe <code>-p</code> tag will map a port from the host to the container.</li> </ul> <pre><code>docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/code/week_1/docker/postgres/data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\n</code></pre> <p>In this case, if you receive an error like <code>initdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty. If you want to create a new database system, either remove or empty the directory \"/var/lib/postgresql/data\" or run initdb with an argument other than \"/var/lib/postgresql/data\".</code> The host folder you are providing is not empty.</p> <p>This will create the folder structure needed for postgres. </p> <p>To interface with the image we use <code>pgcli</code>, a python package that we will add to poetry. <pre><code>poetry add \"psycopg2-binary\"\npoetry add pgcli\n</code></pre> Using the following command we can interface with the database: <pre><code>pgcli -h localhost -p 5432 -u root -d ny_taxi\n</code></pre> This allows us to use commands on the container. </p> <p>We will add jupyter and other packages to our environment to explore the data. <pre><code>poetry add jupyter pandas sqlalchemy\n</code></pre></p> <p>Now we will download the dataset, which is the NY taxi dataset, which is a common one for experimenting with data tools. The one used is this one.</p> <p><pre><code>curl -O -L --output-dir ./code/week_1/docker/postgres/raw https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> or <pre><code>wget -p ./code/week_1/docker/postgres/raw https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre></p> <p>We load this data into Postgres with the code inside this notebook.</p> <p>To see the data and interact with it we are going to use pgAdmin inside a docker container, wich we will be able to see in our browser at <code>http://localhost:8080/</code>.</p> <pre><code>docker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\n</code></pre>"},{"location":"week_1/5-networks/","title":"Networks","text":"<p>Right now the database and pgAdmin will not be able to see each other, we need to put them inside a common network.  <pre><code>\nflowchart\nsubgraph Network\n  subgraph Docker container\n  A[pgAdmin]\n  end\n\n  subgraph Docker container\n  B[(Postgres)]\n  end\nend\nA --- B</code></pre></p> <p>The code to create a network called <code>pg-network</code> is:</p> <p><pre><code>docker network create pg-network\n</code></pre> and adding the network as environmental variable to the other two containers:</p> <pre><code>docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/code/week_1/docker/postgres/data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --name pg-database \\\n  --network=pg-network \\\n  postgres:13\n\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  dpage/pgadmin4\n</code></pre> <p>Tip</p> <p>We gave the database a <code>--name</code> to configure the connection easier, since if you dont you need to check the name docker has given the container.</p> <p>We can now go to <code>localhost:8080</code> since in the pgadmin configuration we passed that as the port in our machine that will comunicate with the container.</p> <p>We will need to login with the credentials <code>admin@admin.com</code> and <code>root</code> which we gave pgadmin as env variables.</p> <p>Now clicking in <code>Add New Server</code>will show us the prompt to pass the database info, add a name and go to <code>Connection</code>.</p> <p></p> <p>Fill out the connection information with your containers data. </p> <p>On the browser to the left we can check the data. Go to <code>Servers &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; Tables</code> right click on it and select <code>First 100 Rows</code>.</p> <p>You can also use SQL code via the <code>Query Tool</code>, accessible on the tools menu or clicking on this symbol: </p> <p></p>"},{"location":"week_1/6-automatic-ingestion/","title":"Automatic Ingestion","text":""},{"location":"week_1/6-automatic-ingestion/#automating-the-pipeline","title":"Automating the pipeline","text":"<p>We are going to refactor the code to automate the ingestion of data into the database.</p> <p>The command to test the pipeline is: <pre><code>python ./code/week_1/docker/automatic_ingestion/data_ingest.py  \\\n  --user=root \\\n  --password=root \\\n  --host=localhost \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre></p> <p>We create the image for the container using the following command and docker file:</p> ConsoleDockerfile <pre><code>docker build -t taxi_ingest:v0.0.1 ./code/week_1/docker/automatic_ingestion\n</code></pre> <pre><code>FROM python:3.9.1\n\nRUN pip install pandas pyarrow sqlalchemy psycopg2-binary\n\nWORKDIR /app\nCOPY data_ingest.py data_ingest.py\n\nENTRYPOINT [ \"python\", \"data_ingest.py\"]\n</code></pre> <p>This allows us to run this container with the following command:</p> <pre><code>docker run -it --network=pg-network taxi_ingest:v0.0.1 \\\n  --user=root\\\n  --password=root \\\n  --host=pg-database \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> <p>Warning</p> <p>Be careful to add the network (so the containers can see each other) and change the <code>--host</code> to <code>pg-database</code> since the hostname of the container is not <code>localhost</code> (our computer) anymore but <code>pg-database</code>.</p>"},{"location":"week_1/7-docker-compose/","title":"Docker-Compose","text":"<p>This is a tool for defining and running multi-container Docker applications, With it, we can start the whole application using only one file.</p> <p>We can specify every parameter of the containers in a file called <code>docker-compose.yaml</code>:</p> <p>Info</p> <ul> <li>Note that we dont need to specify the network, <code>docker-compose</code> automatically puts every service inside the same network.</li> <li>The volumes have a <code>:rw</code> at the end to give read and write permissions to the container in those volumes.</li> </ul> <pre><code>services:\npg-database:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- ../postgres/data:/var/lib/postgresql/data:rw\nports:\n- \"5432:5432\"\n\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\n</code></pre> <p>Now we can call <code>docker-compose</code> to spin up all these services: <pre><code>docker-compose -f ./code/week_1/docker/automatic_ingestion/docker-compose.yml up\n</code></pre></p> <p></p> <p>Tip</p> <p>You only need the <code>-f</code> flag if your <code>docker-compose.yaml</code> is not no your current directory.</p> <p>You can start the services normally with: <pre><code>docker-compose up \n</code></pre> Also you can run them in detached mode if you use the <code>-d</code> flag.</p>"},{"location":"week_1/8-google-cloud-env/","title":"Setting up the Environment on Google Cloud (Cloud VM + SSH access)","text":"<p>We will need a VM instance in Google Cloud that contains everything we need for the project. </p>"},{"location":"week_1/8-google-cloud-env/#create-a-vm-instance","title":"Create a VM Instance","text":"<p>First we generate an ssh key to connect following the documentation.</p> <p>The command for MacOS is: <pre><code>ssh-keygen -t rsa -f ~/.ssh/gcp -C tonivalle -b 2048\n</code></pre> This will generate two ssh keys, one public and on private, inside <code>~/.ssh</code>. Fore more info on ssh keys, click here.</p> <p>Go to <code>Menu Bar &gt; Compute Engine &gt; Metadata</code>:</p> <p></p> <p>Info</p> <p>You may need to enable the Compute Engine API (<code>Menu Bar &gt; Compute Engine</code>) to be allowed to add the ssh key and otherwise use this functionality.</p> <p>In the <code>ssh keys</code> tab you will need to add your public key. All the VM instances you create will inherit these keys. To see the ssh key you can use  <pre><code>cat ~/.ssh/gcp.pub\n</code></pre></p> <p>Go to <code>Menu Bar &gt; Compute Engine &gt; VM Instances</code>:</p> <p></p> <p>You can click on <code>Create Instance</code>and add the configuration depending on your location: </p> <p>Danger</p> <p>We are using a machine of type <code>e2-standard-4 (4 vCPU, 16 GB memory)</code> which costs $0.16 per hour. It is important to shut down the VM when we are not working to not pay more than it is needed.</p> <p>On boot disk we will use Ubuntu 20.04 with 30 GB of memory </p> <p>Now we can connect to the VM via: <pre><code>ssh -i ~/.ssh/gcp &lt;Username inside your key&gt;@&lt;Public IP of your VM&gt;\n</code></pre></p> <p>Another way to connect is to use a ssh config file:</p> Create the ssh configFill the config infoConnect to the VM <pre><code>touch  ~/.ssh/config\ncode  ~/.ssh/config\n</code></pre> <pre><code>Host &lt;Name of your VM&gt;\nHostName &lt;Public IP of your VM&gt;\nUser &lt;Username inside your key&gt;\nIdentityFile ~/.ssh/gcp\n</code></pre> <pre><code>ssh &lt;Name of your VM&gt;\n</code></pre>"},{"location":"week_1/8-google-cloud-env/#work-with-the-vm-inside-vscode","title":"Work with the VM inside VSCode","text":"<p>To allow editing files via ssh we can install the VSCode <code>Remote - SSH</code> Extension. Now, if we click on this button on the bottom left:</p> <p></p> <p>We can use ssh inside VSCode.</p> <p>On the VM we will use:</p>"},{"location":"week_1/8-google-cloud-env/#configure-the-environment","title":"Configure the Environment","text":"<pre><code>sudo apt update\nsudo apt install python3 python3-dev python3-venv\nsudo apt-get install wget\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\nsudo apt-get install docker.io\n</code></pre> <p>We can use docker without having to type sudo every time if we run these commands (You will need to log out of ssh and back in for the membership to aply): <pre><code>sudo groupadd docker\nsudo gpasswd -a $USER docker\nsudo service docker restart\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#pyenv","title":"Pyenv","text":"<p>To configure pyenv (not needed but always a good idea) we can do: <pre><code>curl https://pyenv.run | bash\n\nsudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n</code></pre> And add this to our .bashrc and restart the shell: <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n</code></pre> Execute now: <pre><code>pyenv install 3.9.5\npyenv global 3.9.5\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#poetry","title":"Poetry","text":"<p>We can install the package's dependencies via poetry like in our local machine: <pre><code>pip install poetry\npoetry install\npoetry shell\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#docker-compose","title":"Docker-compose","text":"<p>We need to add docker-compose to the VM. We can do that bu running: <pre><code>mkdir bin\ncd bin\nwget https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64 -O docker-compose # Change de direction to the latest release of docker-compose for your architecture.\nchmod +x docker-compose\n</code></pre> Now we add it to the .bashrc <pre><code>export PATH=\"${HOME}/bin:${PATH}\"\n</code></pre></p> <p>To execute the docker-compose we clone the repo inside our VM and execute it: <pre><code>docker-compose -f ./code/week_1/docker/automatic_ingestion/docker-compose.yml -d up\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#port-forwarding","title":"Port Forwarding","text":"<p>This technique allows us to connect ports in our VM to our local computer.</p> <p>It can be done via this command depending on the direction:</p> If your connection is already establishedRemote port to localhostLocal port to remote <pre><code># Type in the console\n~C\n-R 8080:localhost:80\n-L 17125:localhost:12125\n</code></pre> <pre><code># Establish connections with the ports already forwarded\nssh -R [REMOTE:]REMOTE_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER\n# Example\nssh -R 8080:localhost:80 public.example.com\n</code></pre> <pre><code># Establish connections with the ports already forwarded\nssh -L [LOCAL_IP:]LOCAL_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER\n# Example\nssh -L 3336:db001.host:3306 user@pub001.host\n</code></pre> <p>Alternatively, you can use VSCode to manage it for you:</p> <p></p> <p>With this configuration, we can access the database on the VM in our local command line without having to connect via ssh. <pre><code>pgcli -h 127.0.0.1 -p 5432 -U root -d ny_taxi\n</code></pre></p> <p>Note</p> <p>Notice we changed the host (<code>-h</code>) from localhost to 127.0.0.1</p> <p>Connecting to <code>http://localhost:8080/</code> lets us see pgAdmin like if it was running on our machine.</p> <p>If we also forward port <code>8888</code> we can see jupyter notebooks started on our VM in our local browser (You may need to copy the url in the VM's console since it includes the token).</p>"},{"location":"week_1/8-google-cloud-env/#ingesting-the-data","title":"Ingesting the data","text":"<p>We can run the data ingestion pipeline by using:</p> <pre><code>docker build -t taxi_ingest:v0.0.1 ./code/week_1/docker/automatic_ingestion\n\ndocker run -it --network=automatic_ingestion_default taxi_ingest:v0.0.1 \\\n   --user=root \\\n   --password=root \\\n   --host=pg-database \\\n   --port=5432 \\\n   --db=ny_taxi \\\n   --table_name=yellow_taxi_trips \\\n   --url=https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet\n</code></pre> <p>Note</p> <p>As always, the network name needs to be the same as your other containers in the network.</p>"},{"location":"week_1/8-google-cloud-env/#adding-terraform-to-the-vm","title":"Adding terraform to the VM","text":"<p>We can add terraform to the VM: <pre><code>wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#adding-the-service-account-to-the-vm","title":"Adding the service account to the VM","text":"<p>We created a service account in the first section of the week's folder.</p> <p>To do this we can use SFTP (Secure File Transfer Protocol) to do it. The json file for the service account was inside <code>$HOME/.config/gcloud/</code> in our case.</p> <p><pre><code>cd $HOME/.config/gcloud/\n\n# This will open an sftp console\nsftp dte-vm\n\nmkdir .gc\ncd .gc\nput &lt;Name of your key&gt;.json\n</code></pre> With this your key should be inside your VM.</p> <p>We will need to set this google application credential since we cant OAUTH from the VM. <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/&lt;Name of your key&gt;.json\ngcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\n</code></pre></p>"},{"location":"week_1/8-google-cloud-env/#using-terraform-on-the-vm","title":"Using terraform on the VM","text":"<p>We can do the same in here as we did on our local machine. We will need to init, plan and</p> <pre><code>terraform -chdir=./code/week_1/terraform init\nterraform -chdir=./code/week_1/terraform plan -var=\"project=vast-bounty-142716\" # This is your project id\n</code></pre>"},{"location":"week_1/8-google-cloud-env/#stop-the-vm","title":"Stop the VM","text":"<p>You can stop the VM via the UI for GCP or using  <pre><code>sudo shutdown\n</code></pre></p> <p>Tip</p> <p>Whenever you restart the machine you will need to pass the new IP given to your VM to your ~/.ssh/config.</p>"},{"location":"week_1/8-google-cloud-env/#info-on-shh-keys","title":"Info on shh keys","text":"<p>via ssh.com.</p> <p>The following simple steps are required to set up public key authentication (for SSH):</p> <ol> <li>Private key stays with the user (and only there), while the public key is sent to the server. Typically with the ssh-copy-id utility.</li> <li>Server stores the public key (and \"marks\" it as authorized).</li> <li>Server will now allow access to anyone who can prove they have the corresponding private key. </li> </ol>"}]}